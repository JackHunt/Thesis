% Encoding: UTF-8
%-------------------------GAUSSIAN PROCESS PAPERS-------------------------

@Article{Lawrence2005,
  author     = {Lawrence, Neil},
  title      = {Probabilistic Non-linear Principal Component Analysis with Gaussian Process Latent Variable Models},
  journal    = {J. Mach. Learn. Res.},
  year       = {2005},
  volume     = {6},
  pages      = {1783--1816},
  month      = dec,
  issn       = {1532-4435},
  acmid      = {1194904},
  issue_date = {12/1/2005},
  numpages   = {34},
  publisher  = {JMLR.org},
  url        = {http://dl.acm.org/citation.cfm?id=1046920.1194904},
}

@InProceedings{Rusu2009,
  author    = {Radu Bogdan Rusu and Nico Blodow and Michael Beetz},
  title     = {Fast Point Feature Histograms (FPFH) for 3D Registration},
  booktitle = {in In Proceedings of the International Conference on Robotics and Automation (ICRA},
  year      = {2009},
  doi       = {10.1109/robot.2009.5152473},
}

@Article{LeCun2015,
  author    = {Yann LeCun and Yoshua Bengio and Geoffrey E. Hinton},
  title     = {Deep learning},
  journal   = {Nature},
  year      = {2015},
  volume    = {521},
  number    = {7553},
  pages     = {436--444},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl    = {https://dblp.org/rec/bib/journals/nature/LeCunBH15},
  doi       = {10.1038/nature14539},
  timestamp = {Sat, 20 May 2017 00:24:52 +0200},
}

@InProceedings{Girshick2015_2,
  author    = {Girshick, Ross},
  title     = {Fast R-CNN},
  booktitle = {The IEEE International Conference on Computer Vision (ICCV)},
  year      = {2015},
  month     = {December},
  doi       = {10.1109/iccv.2015.169},
}

@InCollection{Goodfellow2014,
  title = {Generative Adversarial Nets},
  author = {Goodfellow, Ian and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
  booktitle = {Advances in Neural Information Processing Systems 27},
  editor = {Z. Ghahramani and M. Welling and C. Cortes and N. D. Lawrence and K. Q. Weinberger},
  pages = {2672--2680},
  year = {2014},
  publisher = {Curran Associates, Inc.},
  url = {http://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf}
}

@InProceedings{Singer07,
  author    = {Yoram Singer and Nathan Srebro},
  title     = {Pegasos: Primal estimated sub-gradient solver for SVM},
  booktitle = {In ICML},
  year      = {2007},
  pages     = {807--814},
  doi       = {10.1145/1273496.1273598},
}

@Article{He2015,
  author        = {Kaiming He and Xiangyu Zhang and Shaoqing Ren and Jian Sun},
  title         = {Deep Residual Learning for Image Recognition},
  journal       = {CoRR},
  year          = {2015},
  volume        = {abs/1512.03385},
  archiveprefix = {arXiv},
  bibsource     = {dblp computer science bibliography, https://dblp.org},
  biburl        = {https://dblp.org/rec/bib/journals/corr/HeZRS15},
  doi           = {10.1109/cvpr.2016.90},
  eprint        = {1512.03385},
  timestamp     = {Mon, 13 Aug 2018 16:46:56 +0200},
  url           = {http://arxiv.org/abs/1512.03385},
}

@InProceedings{Bromley1993,
  author    = {Jane Bromley and Isabelle Guyon and Yann LeCun and Eduard S{\"a}ckinger and Roopak Shah},
  title     = {Signature Verification Using a Siamese Time Delay Neural Network},
  booktitle = {NIPS},
  year      = {1993},
  doi       = {10.1142/9789812797926_0003},
}

@TechReport{Minka2005,
  author = {Minka, Tom and others},
  title  = {Divergence measures and message passing},
  year   = {2005},
  school = {Technical report, Microsoft Research},
}

@InProceedings{Kendall2015,
  author    = {Kendall, Alex and Grimes, Matthew and Cipolla, Roberto},
  title     = {PoseNet: A Convolutional Network for Real-Time 6-DOF Camera Relocalization},
  booktitle = {Proceedings of the International Conference on Computer Vision ({ICCV})},
  year      = {2015},
  doi       = {10.1109/iccv.2015.336},
}

@Article{Szegedy2014,
  author        = {Christian Szegedy and Wei Liu and Yangqing Jia and Pierre Sermanet and Scott E. Reed and Dragomir Anguelov and Dumitru Erhan and Vincent Vanhoucke and Andrew Rabinovich},
  title         = {Going Deeper with Convolutions},
  journal       = {CoRR},
  year          = {2014},
  volume        = {abs/1409.4842},
  archiveprefix = {arXiv},
  doi           = {10.1109/cvpr.2015.7298594},
  eprint        = {1409.4842},
  url           = {http://arxiv.org/abs/1409.4842},
}

@Article{Pan2010,
  author     = {Pan, Sinno Jialin and Yang, Qiang},
  title      = {A Survey on Transfer Learning},
  journal    = {IEEE Trans. on Knowl. and Data Eng.},
  year       = {2010},
  volume     = {22},
  number     = {10},
  pages      = {1345--1359},
  month      = oct,
  issn       = {1041-4347},
  acmid      = {1850545},
  address    = {Piscataway, NJ, USA},
  doi        = {10.1109/TKDE.2009.191},
  issue_date = {October 2010},
  keywords   = {Transfer learning, Transfer learning, survey, machine learning, data mining., data mining., machine learning, survey},
  numpages   = {15},
  publisher  = {IEEE Educational Activities Department},
}

@Comment{jabref-meta: databaseType:bibtex;}

@Article{Diederik2014,
  author    = {Diederik P. Kingma and
               Jimmy Ba},
  title     = {Adam: {A} Method for Stochastic Optimization},
  journal   = {CoRR},
  volume    = {abs/1412.6980},
  year      = {2014},
  url       = {http://arxiv.org/abs/1412.6980},
  archivePrefix = {arXiv},
  eprint    = {1412.6980},
  timestamp = {Mon, 13 Aug 2018 16:47:35 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/KingmaB14},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@Article{Russakovsky2014,
  author    = {Olga Russakovsky and
               Jia Deng and
               Hao Su and
               Jonathan Krause and
               Sanjeev Satheesh and
               Sean Ma and
               Zhiheng Huang and
               Andrej Karpathy and
               Aditya Khosla and
               Michael S. Bernstein and
               Alexander C. Berg and
               Fei{-}Fei Li},
  title     = {ImageNet Large Scale Visual Recognition Challenge},
  journal   = {CoRR},
  volume    = {abs/1409.0575},
  year      = {2014},
  url       = {http://arxiv.org/abs/1409.0575},
  archivePrefix = {arXiv},
  eprint    = {1409.0575},
  timestamp = {Mon, 13 Aug 2018 16:48:19 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/RussakovskyDSKSMHKKBBF14},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}



@InProceedings{Glorot2010,
  title = 	 {Understanding the difficulty of training deep feedforward neural networks},
  author = 	 {Xavier Glorot and Yoshua Bengio},
  booktitle = 	 {Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics},
  pages = 	 {249--256},
  year = 	 {2010},
  editor = 	 {Yee Whye Teh and Mike Titterington},
  volume = 	 {9},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Chia Laguna Resort, Sardinia, Italy},
  month = 	 {13--15 May},
  publisher = 	 {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf},
  url = 	 {http://proceedings.mlr.press/v9/glorot10a.html},
  abstract = 	 {Whereas before 2006 it appears that deep multi-layer neural networks were not successfully trained, since then several algorithms have been shown to successfully train them, with experimental results showing the superiority of deeper vs less deep architectures. All these experimental results were obtained with new initialization or training mechanisms. Our objective here is to understand better why standard gradient descent from random initialization is doing so poorly with deep neural networks, to better understand these recent relative successes and help design better algorithms in the future.  We first observe the influence of the non-linear activations functions. We find that the logistic sigmoid activation is unsuited for deep networks with random initialization because of its mean value, which can drive especially the top hidden layer into saturation. Surprisingly, we find that saturated units can move out of saturation by themselves, albeit slowly, and explaining the plateaus sometimes seen when training neural networks. We find that a new non-linearity that saturates less can often be beneficial. Finally, we study how activations and gradients vary across layers and during training, with the idea that training may be more difficult when the singular values of the Jacobian associated with each layer are far from 1.  Based on these considerations, we propose a new initialization scheme that brings substantially faster convergence.}
}

@InProceedings{Xiang2014,
  author       = {Yu Xiang and Roozbeh Mottaghi and Silvio Savarese},
  title        = {Beyond PASCAL: A Benchmark for 3D Object Detection in the Wild},
  booktitle    = {IEEE Winter Conference on Applications of Computer Vision (WACV)},
  year         = {2014},
}