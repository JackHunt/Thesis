Much has changed since the introduction of the IBM PC platform in 1981, with regards to the 
uses of the Personal Computer and the way in which humans interact with them. Vast increases 
in computational power have seen the microcomputer evolve beyond a tool with which one predominantly 
collated data with spreadsheets (``VisiCalc'' is widely accepted to be the first \textit{killer app}
for the PC). At present, personal computing is ubiquitous, providing the means for activities ranging 
from playing visually realistic 3D games, to sharing life events online with friends and family via 
so called ``Social Media'' platforms. Advances in graphics processing power have lead to the introduction 
of many technologies that both the public sector and private enterprises have come to depend on, from 
sophisticated medical image analysis, to robotic production lines.

The aforementioned vast power of the modern GPU (Graphical Processing Unit) has lead to the development 
of new ways for humans to interact with computers, and for computers to interact with the world. Advances 
in VR (Virtual Reality) allow humans to experience digital worlds in a fully immersive manner, with 
applications ranging from gaming, to experimental therapeutic treatment of Paranoid Schizophrenia. 
Whereas AR (Augmented Reality) allows a person to supplement their perception of the tangible world with 
elements of the digital, by viewing the world through a computer system that percieves and adds synthetic 
components. Common applications of AR include digital interior design and smartphone games.

Many of the aforementioned advancements in the applications of and interactions with modern computer 
systems are driven by Artificial Intelligence (AI). Modern AI provides much of the semantic and contextual 
information required to make meaningful inferences over the state of the world, as observed by a sensor 
(such as a camera) attached to such a computer system. Much advancement has been made in recent years on 
the tasks of object detection and semantic understanding in standard, 2D images. However, there are many 
technical challenges that must be overcome before such efficacy on these tasks is reached for the 3D 
case. Many modern AI and machine learning algorithms require vast quantities of data to learn to 
perform a given task successfully. This is not prohibitive for systems that operate on 
standard 2D images, due to the abundance of available data. However, for the 3D case there 
is not a comparable volume of 3D data with real world geometric information from which a system 
can learn to perform complex tasks in the real world.

However, there is an established field of computer vision concerned with the geometric processing 
of 3D world observations. Driven by the availibility of consumer grade depth sensing equipment such 
as the Microsoft Kinect RGBD camera (introduced for the Microsoft XBox 360), there has been a renewed 
interest in dense scene reconstruction. Advances in recent years have allowed for the creation of digital 
reconstructions of the tangible world with consumer grade computer equipment. Such systems iteratively 
integrate observed world points into a ``global'' model, such that over time, a smooth representation 
of the observed world surfaces is built. In addition to the integration of such information into a model, 
there is the task of inferring how the sensor has moved in world space, such that the observed points may 
be transformed and integrated into the appropriate model location. The amalgamation of these two tasks is 
known as SLAM (Simultaneous Localisation and Mapping). A typical reconstructrion using such a system is 
given in Figure~\ref{}.

% -- Semantics
% ---- object recognition and segmentation
% ---- user interaction
% -- Amalgamation
% ---- applications in VR/AR
% ---- ad-hoc ML (3d data procurement etc)
The aforementioned advances in semantic segmentation have been utilised within the context of 3D vision to 
introduce a semantic component to dense SLAM systems. Such a combination of techniques provides an adaptable 
component to AR and robotics applications. Early work on amalgamating the two areas of research have allowed 
one to view a reconstruction of their environment in VR and interactively label some of the objects within 
it, with the system inferring the remaining labellings. An example of the output of such a system is given 
in Figure~\ref{}.

Though the results of the systems shown in Figures~\ref{} and~\ref{} represent impressive advancements in 
computer vision, there are however open technical challenges. One such challenge is the successful modelling 
of real environments in which there are dynamic components (such as people walking in the camera's view). 
The traditional dense SLAM pipeline is unable to accurately build a globally consistent model in such 
environments. In addition, when using a combined reconstruction and semantics system such as that shown 
in Figure~\ref{}, many of the descriptive cues that enable the segmentation to be performed rely on 
features utilising 2D image information. As such, there is no ``true'' semantic 3D object learning and 
recognition. As previously outlined, this is a difficult issue to overcome due to the lower volume of real 
world 3D data from which such a system can learn. As such, it becomes neccessary to perform reconstruction 
of objects in isolation to procure such data. Furthermore, within the context of modelling objects in an 
environment, it is not always possible (or practical) to gain full coverage with a sensor of certain objects.

As outlined, traditional dense SLAM systems have difficulty when attempting to perform dense reconstruction 
in an environment where there is motion. The aforementioned senor pose estimation phase in many such systems
is prone to error or failure in such a scenario. The reason for this is due to the reliance on point based 
correspondences between frames. If a static environment is being modelled, then a high number of valid point 
correspondences will be found. However, when motion (independent of the sensors motion) is introduced into 
the scene, invalid correspondences may be found. For example, points that belong to a non moving object 
such as a chair may erroneously be matched to those on a moving scene component, such as a walking person.
Such erroneous correspondences can incur failure cases ranging from moderate model inconsistencies to total 
loss of sensor tracking.

Though there are many use cases for static scene reconstruction, the lack of robustness to dynamics is 
prohibitive in scenarios where a high level of machine perception is required. For example, if 
reconstructing a busy working environment in which there is a high level of dynamics (people walking, 
doors opening etc), an ideal reconstruction would not include artifacts of such motion. As such, the 
reconstruction system would be required to identify such components and account for them in the 
reconstruction process. Additionally, a system that is capable of detecting and segmenting such motion 
would be capable of extracting pertinent semantic information from such events.

% -- Object centric SLAM/SfM
% ---- preliminary work limited on simultaneous TaM; offline or known pose
% ---- fundamentally a difficult problem; less to work with
% ---- geometric inconsistencies more pronounced effect; bad training data etc


% -- Data driven pose/shape regression
% ---- complex environments may not allow long enough to fuse model
% ---- larger scale makes dense reconstruction infeasable
% ---- limited observability; not able to fully cover object
% ---- eliminate many lower level "tricks" to make reconstruction work

% Research questions.
% -- Can we reconstruct densely in dynamic scenes?
% ---- real time?
% ---- comparable reconstruction quality to state of art?
% ---- can we improve tracking in these scenarios?
% -- Can we determine what is dynamic in a scene?
% ---- Can we leverage this information in some semantic way (link back to AR applications/data collection)?
% -- Can we obtain consistent reconstructions of arbitrary objects?
% ---- comparable reconstruction quality to state of the art scene based?
% ---- with commodity hardware?; wider applicability
% ---- without known pose?
% ---- 
% -- Can we infer scene properties where traditional reconstruction not possible?
% ---- shape and pose?
% ---- without requiring temporal consistency?; avert tracking errors
% ---- 

% Thesis roadmap.
% -- Literature review.
% ---- TaM/SLAM
% ---- Semantics/Semantic SLAM
% ---- Dynamics, moseg and optical flow
% ---- Object reconstruction
% ---- Shape and pose regression
% -- "Real Time Motion Segmentation for Dense Volumetric Fusion"
% ---- Outline preliminaries; standard dense SLAM etc
% ---- Approach to dynamic dense SLAM
% ---- Segmentation of dynamics; which questions adressed?
% ---- Rudimentary example of semantics; which questions adressed?
% ---- Outline experiments
% ---- Results showing...
% -- "Probabilistic Object Reconstruction with Online Drift Correction"
% ---- Outline approach; which questions adressed?
% ---- Outline experiments
% ---- Outline results
% -- "Stereo Shape and Pose Regression"
% ---- Outline approach; which questions adressed?
% ---- Outline experiments
% ---- Outline results