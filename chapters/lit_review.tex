%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "../thesis"
%%% End: 

%% Tracking and Mapping.
\section{Tracking and Mapping}
\label{sec:lit_review_tam}
There has been much research in the field of Tracking and Mapping in recent 
years, with many large scale works being driven by the availibility of once 
costly depth sensing equipment. The availability of such equipment combined 
with the ever increasingly parallel nature of modern GPU's has seen the 
field advance greatly beyond the seminal but resource limited works of it's 
infancy. This advancement is most predominant within the Dense SLAM(Simultaneous 
Localisation and Mapping) literature. This section shall first explore the 
earlier, fundamental works of this area of research, followed by a treatment 
of the current state of the art.

\textit{Besl \& McKay} \cite{Besl1992} introduced their seminal work on 3D shape 
registration in 1992, providing a method to estimate full 6DoF(Six Degrees of 
Freedom) pose between 3D point sets. The authors present an Iterative Closest 
Point Algorithm that consists of three operations per iteration; computation 
of closest point, computation of a 6DoF Transformation and application of 
the Transformation. The authors present a proof of convergence based on that 
of Least Squares minimisation, however there must be sufficiently complex 
geometry present in the structure of the data to optimise to a meaningful 
Transformation.

Complementary to the aforementioned works of \textit{Besl \& McKay} \cite{Besl1992} 
in the foundational aspects of Dense SLAM is that of \textit{Curless \& Levoy} 
\cite{Curless1996}, 1996. The authors present an early volumetric integration 
framework for the reconstruction of shapes from range data obtained from a 
sensor such as a laser scanner. The authors introduce the SDF(Signed Distance Function), 
a volumetric, implicit shape representation in which entries are cumulatively updated 
in a weighted manner. Once observations have been integrated in to the SDF volume, an 
Isosurface representation of the shape is extracted by a Marching Cubes \cite{MC} 
procedure. Though the approach may lead to gaps in the resultant model, the authors 
mitigate this by introducing a Surface Tesselation step.

Later work by \textit{Zhou et al} \cite{Zhou2008} in 2008 introduces an alternative 
shape representation to that of \textit{Curless \& Levoy} \cite{Curless1996}, based 
on the spatial KD-Tree data structure and a highly data parallel BFS(Breadth First 
Search) construction algorithm. The level of parallelism introduced allows for 
application to problems that require real time performance. The authors provide 
examples of use in Ray Tracing \cite{RT} and Photon Mapping \cite{PM}.

In the same year, \textit{Censi} \cite{Censi2008} introduced \textit{PL-ICP}, a 
variant of the ICP algorithm introduced by \textit{Besl \& McKay} \cite{Besl1992}. 
The presented approach utilises a Point-to-Line metric rather than a Point-to-Point 
metric and has a closed form solution in the planar case. For the non planar case 
the presented approach acheives quadratic convergence in a finite number of steps, 
utilising a Normal weighting and a Lagrangian optimisation scheme. However, it is 
highlighted that prior to the optimisation procedure it is necessary to trim outliers 
from the point data sets.

Culminating much of the aforementioned work, in 2011 \textit{Newcombe et al} introduced 
the seminal \textit{KinectFusion} pipeline, allowing for real time mapping of indoor 
scenes with the Kinect RGBD sensor from Microsoft. The authors utilise a sparser form 
of the SDF structure introduced by \textit{Curless \& Levoy} \cite{Curless1996}, the 
TSDF(Truncated Signed Distance Function), allowing for reconstruction at scene scale. 
For pose estimation, a multi-level variant of the ICP algorithm utilising a Point-to-Plane 
metric similar to that of \textit{PL-ICP} \cite{Censi2008} is used. The pipeline consists 
of four phases; \textit{Measurement}, \textit{Integration}, \textit{Isosurface Extraction} 
and \textit{Pose Update}. Applications of the presented system however are limited only to 
those that require the reconstruction of static scenes; dynamic scenes are not supported 
by \textit{KinectFusion}.

Further optimisations were made in 2013 by \textit{Nei{\ss}ner at al} \cite{NieBner2013} to the 
\textit{KinectFusion} pipeline proposed by \textit{Newcombe et al}. The authors introduce a 
spatially hashed TSDF data structure, in which the TSDF is split in to hashed Voxel Blocks 
allowing for very fast Voxel lookups. The presented approach yields low space and time complexity 
for such operations, vastly increasing the potential for real time, large scale use. Additionally, 
a streaming system is introduced to dynamically handle data transmission between the CPU and GPU, 
allowing for the reconstruction of scenes that may exceed the GRAM bounds of commodity GPU's. The 
proposed system is capable of running at $\approx46Hz$ on an NVIDIA Titan GPU.

In the same year, \textit{Thomas et al} \cite{Thomas2013} introduced an alternative scene 
representation, based on the notion that a scene may be represented as a set of Planar 
components with attributes such as Surface Normals, confidences and RGB colour. The motivation 
of the author's approach is that many common scenes that one might reconstruct are indoors and 
consist of components that are planar in nature, such as walls, floors and ceilings. Additionally, 
many planar objects are common, such as tables and cabinets. The author presents an alternative 
rendering approach based on Quadrangulation \cite{QUAD} and utilise a \textit{KinectFusion} 
\cite{Newcombe2011} like ICP based algorithm for pose estimation.

\textit{Salas-Moreno et al} \cite{Salas-Moreno2013} in 2013 also, introduced another alternative 
approach to that of the \textit{KinectFusion} \cite{Newcombe2011} like pipelines that similarly to 
\textit{Thomas et al} \cite{Thomas2013}, utilises the prior information that many scenes consist 
of predictable, repeated structures. As such, the authors introduce a so called ``Object Oriented'' 
Dense SLAM paradigm, in which the reconstruction of the scene is split in to a graph of observed 
objects. Pose estimation is achieved by running an ICP based algorithm against renderings of the 
individual objects in the reconstructed scene model. Following pose estimation, the proposed 
system detects newly observed objects and inserts the appropriate object model in to the scene 
model. Consistency between scene components is enforced with Pose Graph Optimisation, with 
relocalisation achieved in a similar manner. The proposed approach does however require a 
database of known objects a-priori.

\textit{St{\"u}ckler et al} \cite{Stuckler2014} in 2014 introduced a non implicit, non volumetric 
representation based on multiple resolution Surfel \cite{Pfister2000} Maps. The core data structure 
used for scene representation is a Voxel Octree \cite{OCTREE} containing both Surfels and 
Probability Distributions over appearance and shape. Pose estimation is achieved by optimising for 
a Unit Quaternion \cite{QUAT} and Translation Vector within a Maximum Likelihood framework, in which 
the Energy Function to be maximised is the Likelihood of the RGBD observations given the accumulated 
Probability Distributions stored in the Octree. The presented pipeline also incorporates a randomised, 
graph and keyframe based loop closure component.

Following the approach of \textit{Thomas et al} \cite{Thomas2013}, \textit{Salas-Moreno et al} 
\cite{Salas-Moreno2014} in 2014 introduced another reconstruction system that utilises the
planarity property of many common scenes. The proposed approach focuses on the detection and 
modelling of planes in the scene, proceeding with their refinement over time. 
The proposed approach generates Surfel \cite{Pfister2000} Maps from observed RGBD frames, from 
which the planar regions are detected and integrated, filling holes in the reconstruction over time.
The authors utilise an ICP algorithm to register the vertex maps of the RGBD observations and the 
reconstructed model. Additionally, relocalisation is achieved by the use of Fern Encoding \cite{FERN}. 

\textit{Prisacariu et al} \cite{Prisacariu2014, Kahler2015} in 2014 followed up the optimisations 
to the \textit{KinectFusion} pipeline proposed by \textit{Nei{\ss}ner et al} \cite{NieBner2013}. 
The authors presented in addition to the original publication, a Technical Report and an Open 
Source implementation. The proposed further improvements to those of \textit{Nei{\ss}ner et al} 
\cite{NieBner2013} include a number of low level optimisations to the core Hashed TSDF data 
structure, it's allocation and update(integration of observation points) and the rendering 
phase of the pipeline. In addition, the authors demonstrate that pose estimation quality may 
be greatly improved by the use of commodity IMU(Inertial Motion Unit) devices, commonly found 
on mobile phones and tablet computers. \textit{Prisacariu et al} report runtimes of 
$\approx47Hz$ on an NVIDIA Shield Tablet and $\approx910Hz$ with a commodity NVIDIA Titan X GPU.

\textit{Whelan et al} \cite{Whelan2015} in 2015 proposed another \textit{KinectFusion} 
\cite{Newcombe2011} like pipeline intended to enable reconstruction of large scale scenes, 
acheiving reconstruction over hundreds of metres. The approach taken by the authors to enable 
such large scale reconstructions is centered around the use of a GPU Cyclic Buffer. For pose 
estimation, the authors impose both geometric and photometric constraints on the camera pose. 
Additionally, the proposed approach performs map updates in an as-rigid-as-possible \cite{ARAP} 
manner, combining frame recognition such that on a recognition event, a map update is performed.
The proposed pipeline provides loop closure capabilites by utilising Pose Graph Optimisation 
\cite{PGO}.

\textit{Zhou et al} \cite{Zhou2015} also in 2015 proposed another variant of the 
\textit{KinectFusion} \cite{Newcombe2011} pipeline proposed by \textit{Newcombe et al}. 
The authors present improvements on the pose estimation phase of the pipeline, utilising 
contour cues to aid association and enforcing correspondence constraints on the estimated 
pose, w.r.t scene geometry. Central to the presented approach is the depth image preprocessing 
steps of inpainting regions of the depth image for which there are no depth measurements, 
followed by the aforementioned contour extraction stage.

The optimised pipeline proposed in 2014 by \textit{Prisacariu et al} was in 2016 improved with 
the addition of loop closure handling by \textit{Kahler et al} \cite{Kahler2016}; \textit{Kahler} 
being one of the authors of the original 2014 contribution. Drift correction is acheived by the 
use of a multiple scene representation, with online alignment being performed periodically 
between the scenes. Corrections between the scenes are made via the use of Pose Graph Optimisation. 
Loop closures are detected by the use of Fern Conservatories \cite{GLOCKER}.

%% Semantic SLAM.
\section{Semantic SLAM}
Over the years there has been much interest within the Computer Vision research community on 
the Semantic understanding of our environment. The ability of machines to recognise and extract 
information about their environments and the components of them has wide application potential, 
ranging from autonomous robotics to augmented reality video games. The application potential 
of this Semantic Scene Understanding ability is amplified when it is combined with the vast 
progress that has been made in Dense SLAM. This section shall provide a survey of research 
that amalgamates the two fields of Semantic Scene Understanding and SLAM.

Bengio et.al. \cite{Bengio2013} - representation learning.
Girshick et.al \cite{Girshick2014} - feature hierarchies.

\label{sec:lit_review_semantic}
\textit{Civera et al} \cite{Civera2011} in 2011 introduced an approach to Semantic SLAM 
that utilises image based features to attach semantic meaning to 3D observations. The SLAM 
system itself is based on Monocular EKF SLAM with semantics added to points via 
correspondences between SURF image features extracted from the observed RGB frames and 
precomputed object descriptors. Consistency is then enforced by a geometric compatibility 
measure.

\textit{St{\"u}ckler et al} \cite{Stuckler2012} in 2012 presented a Semantic Dense SLAM 
pipeline for the object centric integration of RGBD images. Given an RGBD frame, objects 
are detected using a Random Forest classifier trained on hand crafted features extracted 
from RGBD images. The proposed approach does not reconstruct an entire scene, rather it 
reconstructs scene components(such as objects) that have been semantically segmented 
from the current RGBD frame.

\textit{Valentin et al} \cite{Valentin2015} in 2015 proposed a fully integrated Dense SLAM 
and Semantic Scene Understanding pipeline with interaction being a primary focus. The 
proposed pipeline at it's core is based on that of \textit{KinectFusion} \cite{Newcombe2011}, 
so requires the use of RGBD images and is restricted to the reconstruction of static scenes. 
Once a scene has been reconstructed, the author's pipeline allows users to interact with 
objects in the scene to provide training data for Streaming Random Forests\cite{Abdulsalam2007}, 
which are used to detect and label parts of the rendered Isosurface belonging to a given 
object class. Segmentations are refined using Variational Bayesian Mean Field Inference 
\cite{Xing2002} optimised by \cite{Krahenbuhl2011}. The features extracted for this training 
process are named \textit{Voxel Oriented Patch} features and consist of Surface Normals 
and CIELab appearance information.

\textit{Golodetz et al} \cite{Golodetz2015} in the same year released an Open Source 
implementation of the pipeline proposed by \textit{Valentin et al} \cite{Valentin2015}, 
utilising the implementation of the \textit{KinectFusion} \cite{Newcombe2011} pipeline 
provided by \textit{Prisacariu et al} \cite{Prisacariu2014}. The framework proposed by 
the authors extends that of \textit{Valentin et al} \cite{Valentin2015} greatly, for 
example by supporting the use of Motion Capture systems and Virtual Reality headsets. 
In addition, the implementation provided is optimised to allow for real time use.

\textit{Handa et al} \cite{Handa2015}, again in 2015 introduced an alternative, 
real time Dense Semantic SLAM pipeline. Much like the approaches of \textit{Valentin et al} 
\cite{Valentin2015} and \textit{Golodetz et al} \cite{Golodetz2015}, the proposed system 
is based on the \textit{KinectFusion} \cite{Newcombe2011} pipeline, with semantic scene 
understanding performed on the rendered Isosurface. Contrary to previous approaches however, 
the authors make use of stacked Deep Autoencoders \cite{DAA}, trained on synthetic depth 
images a priori. As such, the proposed system makes use only of depth cues and may not be 
adapted to new object classes on an ad-hoc basis.

\textit{Cavallari et al} \cite{Cavallari2016} in the following year presented another 
Semantic Dense SLAM pipeline built on top of the Dense SLAM system presented by 
\textit{Nei{\ss}ner et al} \cite{NieBner2013}. Much like the work of \textit{Handa et al} 
\cite{Handa2015}, the proposed approach depends on a model pretrained on a set of object 
classes. Unlike \textit{Handa et al} \cite{Handa2015}, the authors make use of an 
FCN(Fully Connected Network) \cite{FCN}, taking the PMF output to determine the class 
to be assigned to an Isosurface region.

%% Dynamic SLAM.
\section{Dynamic SLAM, Motion Segmentation and Optical Flow}
\label{sec:lit_review_dynamic}
Sections \ref{sec:lit_review_tam} and \ref{sec:lit_review_semantic} provided an assesment 
of pertinent literature on the topics of SLAM and Semantic SLAM. However, all of the 
aproaches outlined in these sections are limited to use in static scenes only without 
the capability to accurately operate in an environment that contains moving objects. This 
section shall explore pertinent literature on the topics of Dynamic SLAM, Motion Segmentation 
and Optical Flow. As such, the general focus of the work surveyed in this section is the 
detection, estimation and segmentation of motion in dynamic scenes.

\textit{Tsap et al} \cite{Tsap2000} in 2000 presented an Algorithm for nonrigid motion 
tracking of objects. The presented approach solves for dense motion Vector Fields between 
3D objects by modelling motion with Finite Elements. The proposed system analyses differences 
between actual and predicted behaviour, using iterative descent optimisation to find a set of 
optimal parameters for the nonlinear FEM(Finite Element Model). Additionally, pose estimation 
is improved by using point correspondences.

\textit{Chen et al} \cite{Chen2011} in 2011 introduced a system to perform nonrigid motion 
tracking of the human body. The proposed system extracts and skins a surface mesh from 
multi-view video, after being fitted with a skeleton prior. To solve for nonrigid, articulated 
motion, the authors utilise a Weighted Hierarchical ICP algorithm, where weightings are obtained 
by the Approximate Nearest Neighbour \cite{ANN} algorithm.

In the following year, \textit{Sun et al} \cite{Sun2012} proposed an approach to motion estimation 
for objects in images. The proposed approach performs Optical Flow in a layered manner, where each 
layer pertains to an object undergoing Rigid Body Motion, with the number of layers being determined 
automatically. The authors utilise Max Flow \cite{MAXFLOW} to solve as discretised flow field cost 
function for each layer, where object layers are a set of depth ordered MRF's(Markov Random Field).

\textit{Unger et al} \cite{Unger2012} again in 2012, proposed an alternative system for the 
estimation of motion of objects undergoing Rigid Body Motion in images. The authors present a 
Variational formulation for motion estimation and segmentation with occlusion handling. As with 
the contributions of \textit{Sun et al} \cite{Sun2012}, the authors utilise a parametric labelling 
of the flow field for each object undergoing motion, with labels encoded with an MRF Potts model.
However, contrary to \textit{Sun et al} \cite{Sun2012} who utilise a Max Flow algorithm over the 
MRF models, \textit{Unger et al} solved for flow and labels within a Primal-Dual based optimisation 
framework.

In the same year, \textit{Herbst et al} \cite{Herbst2013} proposed an extension to Optical 
Flow to 3D scenes; the proposed system solves for Scene Flow based on RGBD data. The proposed 
approach is similar to that of \textit{Brox et al} \cite{Brox2004}, with Scene Flow being 
formulated as a Variational Optimisation problem. The presented approach is a generalisation 
of the well established Variational Optical Flow algorithm of \textit{Brox et al} \cite{Brox2004}.

In the following year, \textit{St{\"u}ckler et al} \cite{Stueckler2013} presented a framework 
for the segmentation of Rigid Body Motion from RGBD data. The authors represent regions undergoing 
Rigid Body Motion and their associated motion parameters as latent variables, with the resultant 
segmentations and parameters being solve for within an EM(Expectation Maximisation) framework. 
The presented approach is robust to both simultaneous foreground and background motion by giving 
each parity.

Though the motion estimation and segmentation approaches reviewed up to this point have not 
been within the SLAM framework, \textit{Keller et al} \cite{Keller2013} in 2013 introduced an 
RGBD based Dense SLAM system capable of performing Motion Segmentation. Unlike the \textit{KinectFusion} 
\cite{Newcombe2011} inspired Dense SLAM pipelines, the presented approach does not utilise an 
implicit, volumetric representation. Rather, the authors opt for an explicit Surfel \cite{Pfister2000} 
based representation. Whilst performing live reconstruction, the proposed system detects and uses ICP 
outliers to determine dynamic scene components. With the information gained from detecting ICP outliers, 
the proposed system then propagates thses detections by a flood fill operation. As the proposed approach 
utilises an explicit, flat data structure for scene representation, it does not have advantages of 
it's highly optimised volumetric counterparts, such as that proposed by \textit{Prisacariu et al} 
\cite{Prisacariu2011}. As such, scalability is limited.

In 2015, \textit{Perera et al} \cite{Perera2015} presented an approach to Motion Segmentation in 
TSDF Volumes. Similar to it's planar counterparts presented by \textit{Sun et al} \cite{Sun2012} 
and \textit{Unger et al} \cite{Unger2012}, the authors utilise a Markov Network over the domain 
of interest. Motion Segmentation is posed as a MAP(Maximum a Posteriori) inference problem over 
a CRF(Conditional Random Field) defined over TSDF Voxels. The proposed system is able to segment 
objects undergoing both minor and major displacements, with motion labels and parameters found 
w.r.t. the live frame and the TSDF. However, the proposed approach is limited only to very small 
scenes, with very long runtimes reported for TSDF Volumes of dimensionality $256^{3}$.

\textit{Newcombe et al} \cite{Newcombe2015} again in 2015, introduced a Dynamic Dense SLAM 
system based on the earlier \textit{KinectFusion} \cite{Newcombe2015} pipeline, with the addition 
of the ability to handle non-rigidly deforming scenes. Nonrigid deformations are handled by the 
estimation of a 6DoF motion field that warps model represented by the TSDF to live frame. The 
solving of the warp field is acheived by the use of Dual Quaternion blending \cite{Kavan2006}. 
Though promising results are presented, there are limitations, such as lack of robustness to 
open/closed topology changes(hands). In addition, the authors highlight scalability issues.

%% Object Reconstruction.
\section{Object Reconstruction}
\label{sec:lit_review_obj_recon}
It is evident from Sections \ref{sec:lit_review_tam}, \ref{sec:lit_review_dynamic} and 
\ref{sec:lit_review_semantic} that much progess has been made in the fields of TAM/SLAM, 
Dynamic SLAM and Semantic SLAM. However, Object Reconstruction remains a very open and active 
field of research. This Section provides a review of pertinent literature on the task of 
reconstructing consistent models of objects, rather than full scale scenes. The problem of 
interest in this section, though related to SLAM, incurs additional complications with regards 
to pose estimation.

\textit{Curless \& Levoy} \cite{Curless1996} as introduced in Section \ref{sec:lit_review_tam} 
presented a method of statically reconstructing shapes from range images taken from different 
viewpoints. However, the presented approach predates many of the advancements that have allowed 
for simultaneous Tracking and Mapping.

\textit{Kolev et al} \cite{Kolev2006} in 2006 presented a probabilistic approach to 3D shape 
segmentation and recovery. Rather than the direct reconstruction approach taken by 
\textit{Curless \& Levoy} \cite{Curless1996}, the authors take the approach of inferring the most 
probable shape w.r.t. the observed image sequences. The shape to be inferred is encoded as a Zero 
Level Set, extracted from a Level Set representation(such as an SDF). The Level Set of the shape is 
evolved over time within a Variational framework, w.r.t. a volume of segmentation probabilities(foregound 
vs background). However, the proposed approach does not have an additional pose estimation phase and has 
only been evaluated on synthetic data of very polarised appearance.

\textit{Weise et al} \cite{Weise2009} in 2009 proposed an approach to the in hand scanning of 3D objects. 
The authors utilise an explicit Point Cloud representation of shape, rendered as Surfels \cite{Pfister2000}. 
Objects are rotated in front of a sensor with poses recovered by the use of an ICP like algorithm. During 
pose estimation, a Topology graph is built which is used to offset drift in estimated poses in an 
as-rigis-as-possible \cite{ARAP} manner. However, the specification of object rotation in front of a 
sensor is suggestive of limited tracking ability. In addition, the type of sensor is not specified, 
as such it is not clear what quality of sensing equipment is required to yield high quality results.

In 2013, \textit{Ren et al} \cite{Ren2013} poposed an approach to the tracking and reconstruction of 
objects. Like \textit{Kolev et al} \cite{Kolev2006}, the authors utilise a probabilistic formulation 
based on the evolution of a Level Set representation. Initialised with a Shape Prior Level Set, the 
proposed approach evolves the Shape Prior w.r.t. observations. Crucially, unlike the approach of 
\textit{Kolev et al} \cite{Kolev2006}, the proposed approach simultaneously optimises for object pose.
The proposed system works with RGBD data and segments the object of interest using Pixel Wise Posteriors 
\cite{Bibby2008}. It is noteworthy however that there are performance limitations of the proposed approach 
and experiments show success for a limited set of target shapes.

Later in 2015, \textit{Dou et al} \cite{Dou2015} present a system for the reconstruction of deformable 
objects using a Microsoft Kinect RGBD sensor. The proposed approach solves for a latent target shape 
and shape deformations by utlising Bundle Adjustment \cite{BA}. The authors report that loop closures 
are automatically detected, with errors incurred by drift being distributed backwards from the detection 
point. The resultant shape surface is extracted as a triangular mesh. The presented experiments demonstrate 
high quality reconstruction results, but with overnight run times and so is not suitable for real time use.

In the following year, \textit{Gupta et al} \cite{Gupta2016} proposed a system for the reconstruction and 
segmentation of 3D objects from data obtained from an RGBD sensor. The reconstructed object is represented 
implicitly within an SDF volume, but notably observations are integrated using the Softmax function rather 
than weighted means as with \textit{KinectFusion} \cite{Newcombe2011}. Each Voxel in the volume is 
assigned a label pertaining to it's membership of the object set, with objects refined utilising Graph 
Cuts and Alpha Expansions. The proposed approach utilises a photometric loss to optimise for object pose, 
with keyframe based loop closure detection. However, the authors report difficulties in building 
sufficiently granular reconstructions. In addition, the authors report drift in pose estimation to be 
problematic.

%% Object Shape Prediction.
\section{Shape and Pose Prediction}
\label{sec:lit_review_prediction}
Section \ref{sec:lit_review_obj_recon} provided a review of pertinent Object Reconstruction research, 
which demonstrates that although much progress has been made since the early work of \textit{Curless \& 
Levoy} \cite{Curless1996}, many open research problems remain. This section provides a survey of research 
into an alternative, inference driven approach to obtaining three dimensional models of observed 
objects, whereby rather than direct optimisation and integration being used for pose estimation and 
model building, the process is posed as a Probabilistic Inference procedure.

\textit{Prisacariu et al} \cite{Prisacariu2011} in 2011 introduced an approach to shape prediction, 
segmentation and pose estimation. Shape is predicted from a Hierarchy of generative Gaussian Process 
Latent Space Models \cite{Lawrence2005}, encoding a Latent Space embedding of common shape properties. 
Candidate shapes are generated as a one off regression in latent space, with a unified Energy Function 
optimised w.r.t. the shape latent space point and the object pose parameters.

In 2013, \textit{Dame et al} \cite{Dame2013} proposed an approach to dense object reconstruction from a 
monocular image source. Like the approach of \textit{Prisacariu et al} \cite{Prisacariu2011}, the authors 
utilise Gaussian Process Latent Variable Models as a Shape Prior to aid reconstruction and segmentation 
of the object of interest. Depth maps for the observed monocular sequence are optimised for within a 
Primal Dual framework, utilising TV(Total Variation) regularisation. However, there is no pose estimation 
ability in the formulation, as poses are known a priori from PTAM \cite{PTAM}.

Toshev et.al. \cite{Toshev2014}
\begin{itemize}
	\item Human pose estimation(articulated estimation) with Deep Neural Networks.
	\item Cascaded DNN regressors.
	\item No reconstruction, pose estimation only.
\end{itemize}

Wohlhart et.al. \cite{Wohlhart2015}
\begin{itemize}
	\item 3D object detection and pose recovery.
	\item CNN descriptors used with Nearest Neighbour Cost for detection and rough pose.
	\item Problem posed as KNN search in Descriptor Space.
	\item Object and Pose are coupled in training(i.e. two similar cars with different poses have distant descriptors)
\end{itemize}

Chang et.al. \cite{Chang2015}
\begin{itemize}
	\item Large scale dataset of 3D shapes.
	\item Synthetic data with no "source" sequence, i.e. no depth.
	\item Can be used to learn rich latent space embeddings.
\end{itemize}

Rock et.al. \cite{Rock2015}
\begin{itemize}
	\item Recovers a complete 3D model from a depth image of an object.
	\item Input depth image matched to database of objects via a Random Forest.
	\item Matched shape coarsely matched to depth map, then deformed at a higher granularity.
	\item Deformation manually optimised for.
\end{itemize}

Zhou et.al. \cite{Zhou2017_2}
\begin{itemize}
	\item Object detection from 3D Point Clouds.
	\item End-to-end trainable Convolutional, Region Proposal Network.
	\item Trained on KITTI LIDAR dataset. \cite{Geiger2013}
	\item Voxelisation of point cloud used for region detections.
\end{itemize}

Gwak et.al \cite{Gwak2017}
\begin{itemize}
	\item GAN like shape prediction with log-barrier ojective.
	\item Weakly supervised with sillhouettes and 3D shapes.
	\item May not work "in the wild"
\end{itemize}

