%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "../thesis"
%%% End: 

\section{Introduction}
\label{sec: moseg_introduction}
Progress in Dense Volumetric Fusion has been accelerated in recent years with
the availability of comsumer grade RGBD sensors such as the Microsoft Kinect and
the Asus Xtion coupled with the increasingly parallel nature of GPU hardware.
Systems such as the seminal KinectFusion \cite{Newcombe2011} allow one to build
high quality, globally consistent scene models trivially. Applications of such
reconstruction pipelines however are limited due to the inability of such
systems to handle scenes in which there are dynamics; such systems are unable
to yield reliable reconstructions when there is motion in the sensors field
of view independent of the sensors own motion. Such a scenario introduces
additional error to the Pose Estimation component in the pipeline and as such
causes model corruption.

In this chapter an approach to solving the problem of performing robust
Dense Volumetric Reconstruction in dynamic scenes is presented. Central to the
pipeline is the introduction of a dual scene representation based on the use
of an implicit TSDF \cite{Curless1996}. The use of a dual TSDF approach allows
for the segmentation of moving components in the scene from static components,
e.g. segmenting a person getting up from a chair from the chair itself. One of
the two scene representations is the ``static'' scene and the other the
``dynamic'' scene. Such separation prevents corruption in the static scene,
the reconstruction output of the system.

Without the segmentation of dynamic scene components, when tracking against the
current reconstruction the integrated dynamic components may cause artifacts
that prevent the finding of ICP correspondences which often causes camera
tracking to drift, or completely fail. By tracking against only the stable
scene components, this inteference in the ICP pose estimation process can be
mitigated.

Once a part of the dynamic model has been stable for a sufficient period, it's
volumetric data is integrates in to the static model and it is used for the
tracking phase of the pipeline. The use of Volumetric structures in this work
is motivated by previous works on Voxel Block Hashing \cite{NieBner2013},
providing efficient, real time lookup operations. The presented approach
exploits the abstraction that Voxel Blocks provide; a block of voxels is
interpreted as a region of space that can be either static or dynamic.
From a survey of the literature, it appears that this approach is the first to
utilise such a dual representation for the motion segmentation problem.

The remainder of this chapter is structured as follows.
Section \ref{sec:moseg_related_work} presents an assessment of related and
relevant literature. Section \ref{sec:moseg_static_fusion} introduces
preliminaries pertaining to the static Fusion pipeline followed by Section
\ref{sec:moseg_dynamic_fusion} describing the dynamic Fusion component of the
pipeline. Qualitative and Quantitative results are presented in Sections
\ref{sec:moseg_qualitative} and \ref{sec:moseg_quantitative}, respectively.
Finally, an application to interactive object recognition is given in Section
\ref{sec:moseg_semantic}.

\section{Related Work}
\label{sec:moseg_related_work}

\section{Static Volumetric Fusion}
\label{sec:moseg_static_fusion}
The Volumetric Fusion approach taken in this work draws on previous Volumetric
integration techniques \cite{Curless1996, Newcombe2011, NieBner2013, Prisacariu2014}
and shall be introduced in this section as preliminary material and shall be
referred to in later chapters. Following this approach, at each frame the camera
is tracked against the current scene, after which new data is fused into the
scene model which is then rendered using Raycasting to prepare for tracking in
the next frame.

The static Fusion pipeline consists of the following three consecutive steps:
\begin{itemize}
  \item Camera Tracking.
  \item Model Integration.
  \item Rendering.
\end{itemize}

The approach in this work utilises the TSDF Volumetric data structure which
encodes for each voxel in the structure, a signed value and a weight.
In the case of 3D environment modelling, the values pertain to distances from
surfaces, within some truncation region.

Given a vertex map generated from the back projection of the points in a depth
image, the vertices pertain to the zero crossing point with Voxels either side
representing distances to the zero crossing point; positive in front of the
surface, negative behind. Surface points are known as the Zero Level Set.
% TO-DO: make diagram.
For a given TSDF $\mathbf{\Phi}$, the Zero Level Set is defined as follows
\begin{equation}
  \label{eqn:zero_level_definition}
  \mathcal{S} = \{v \given \mathbf{\Phi}(v) = 0\}, 
  \mkern2mu \forall \mkern2mu v \in \mathbf{\Phi}
\end{equation}
where $v$ denotes a TSDF Voxel.

To facilitate real time fusion, the InfiniTAM framework employs a Voxel Block
Hashing mechanism for fast access to scene Voxels \cite{NieBner2013}. Within
this context, Voxel Blocks are collections of $N \times N \times N$ TSDF Voxels,
stored in a Hash Table for fast access. As such, each Hash Table entry
corresponds to a portion of a global Voxel Block Array, pertaining to a region
in the scene. This division of the scene into Voxel Blocks is used for the later
process of determining and labelling dynamic regions in the scene.

\subsection{Camera Tracking}
\label{subsec:moseg_static_camera_tracking}
As in previous works \cite{Newcombe2011, Prisacariu2014}, the gradient
optimisation based ICP algorithm is utilised to register consecutive images
to derive the camera pose at time $t$ with respect to time $t-1$, that is to
optimise for the Rigid Body Transform $\mathbf{T} \in \mathbb{SE}(3)$ of the
camera between the two frames using the Levenberg-Marquardt Nonlinear Least
Squares method \cite{NumericalRecipes}. The rendering stage of the pipeline
is used to generate the image from the TSDF  at time $t-1$ to which a new frame
at time $t$ is registered.

The target Transformation $\mathbf{T} \in \mathbb{SE}(3)$ is a member of the
Special Euclidean Group
\begin{equation}
  \label{eqn:se3_definition}
  \mathbb{SE}(3) = \{\mathbf{R}, \mathbf{t} \given \mathbf{R} \in
  \mathbb{SO}(3), \mathbf{t} \in \mathbb{R}^{3}\}
\end{equation}
and has the following form
\begin{equation}
  \label{eqn:trans_mat_definition}
  \mathbf{T} =
  \begin{bmatrix}
    \mathbf{R} & \mathbf{t} \\
    \mathbf{0} & 1
  \end{bmatrix}
\end{equation}
where $\mathbb{SO}(3)$ is the Special Orthogonal Group of Skew Symmetric
Rotation Matrices.

\subsubsection{Attitude Representation}
\label{subsub:moseg_static_camera_attitude}
The Rotation Matrix component of the Transformation $\mathbf{T}$ is
Generated by a Rodriguez Paramaterization\cite{Shuster1993}, whereby the
$\mathbb{SO}(3)$ Rotation Matrix $\mathbf{R}$ is generated by three rotational
parameters, $\alpha$, $\beta$ and $\gamma$. Each parameter represents a rotation
around one of three Principal Axes.

The formulation of the Rodriguez Paramaterization is given by the following 
\cite{Shuster1993}, with the parameter Vector
$\mathbf{p} = [\alpha, \beta, \gamma]^{T}$
\begin{equation}
  \label{eqn:rodriguez_matrix_eq}
  \mathbf{R}(\mathbf{p}) =
  \frac{1}{\norm{\mathbf{p}}^{2}}
  \bigg[
  (1 - \norm{\mathbf{p}}^{2})\mathbf{I} +
  2 \mathbf{pp}^{T} + \omega(\mathbf{p})
  \bigg]
\end{equation}
where for an arbitrary Vector $\mathbf{v}$, $\omega(\mathbf{v})$ is defined as
the Cross Product Matrix operator and is defined as follows
\begin{equation}
  \label{eqn:cross_prod_mat}
  \omega(\mathbf{v}) =
  \begin{bmatrix}
    0 & v_{3} & -v_{2} \\
    -v_{3} & 0 & v_{1} \\
    v_{2} & -v_{1} & 0
  \end{bmatrix}
\end{equation}

Evaluating Equation \ref{eqn:ridriguez_matrix_eq} leads to the following form
of the Rotation Matrix $\mathbf{R}$
\begin{equation}
  \label{eqn:rodriguez_matrix}
  \mathbf{R} = \frac{1}{\norm{\mathbf{p}}^{2} + 1}
  \begin{bmatrix}
    % Row 1.
    \alpha^{2} - \beta^{2} - \gamma^{2} + 1 &
    2 \alpha \beta + \gamma &
    2 \alpha \gamma - \beta \\
    % Row 2.
    2 \alpha \beta - \gamma &
    -(\alpha^{2} - \beta^{2} + \gamma^{2} - 1) &
    \alpha + 2 \beta \gamma \\
    % Row 3.
    2 \alpha \gamma + \beta &
    -(\alpha - 2 \beta \gamma) &
    -(\alpha^{2} + \beta^{2} - \gamma^{2} - 1)
  \end{bmatrix}
\end{equation}

The Translational component $\mathbf{t}$ of the Transformation $\mathbf{T}$ is
given by the following Vector, with each component representing a translation
along it's respective axis.
\begin{equation}
  \label{eqn:trans_vector}
  \mathbf{t} =
  \begin{bmatrix}
    t_{x} \\
    t_{y} \\
    t_{z}
  \end{bmatrix}
\end{equation}

\subsubsection{Pose Recovery Formulation}
\label{subsub:moseg_static_camera_poserec}
The recovery of the camera pose change between frames $t$ and $t-1$ may be
formulated as the following Energy Minimisation problem
\begin{equation}
  \label{eqn:pose_estimation_kf}
  E(\mathbf{R}, \mathbf{t}, \mathbf{\Omega}, \mathbf{\Phi}) =
  \argmin_{\mathbf{R}, \mathbf{t}} \sum_{p \in \mathbf{\Omega}}
  \norm{\left[
    \mathbf{Rx} + \mathbf{t} - \mathcal{V}(\bar{\mathbf{x}})
  \right]^{T}
  \mathcal{N}(\bar{\mathbf{x}})}
\end{equation}
where $\mathbf{R}$ and $\mathbf{t}$ are the aforementioned Rotation Matrix and
Translation Vector of the transformation $\mathbf{T}$. $\mathbf{x}$ is the 3D
point extracted from the depth image $\mathbf{\Omega}$ and the point
$\bar{\mathbf{x}}$ is the 3D point in the TSDF Volume $\mathbf{\Phi}$ found by
Raycasting from $\mathbf{\Omega}$ under the Transformation $\mathbf{T}$.
Finally, $\mathcal{N}$ is a Normal map of $\mathbf{\Phi}$ and is defined as
follows
\begin{equation}
  \label{eqn:normal_map_definition}
  \mathcal{N} = \frac{\nabla \mathbf{\Phi}}{\norm{\nabla \mathbf{\Phi}}}
\end{equation}
where $\nabla \mathbf{\Phi}$ is approximated with Central Finite Differencing.

For the Gradient update phase of the ICP algorithm, the Partial Derivatives
$\frac{\partial E}{\partial \mathbf{R}_{\{\alpha, \beta, \gamma\}}}$ may be
derived as follows in Equation \ref{eqn:icp_update_derivation}.
As a first step, the following definition is made
\begin{equation}
  \label{eqn:icp_deriv_sub}
  \phi(\mathbf{R}, \mathbf{t}, \mathbf{x}, \bar{\mathbf{x}}) =
  \left[
    \mathbf{Rx} + \mathbf{t} - \mathcal{V}(\bar{\mathbf{x}})
  \right]^{T}
  \mathcal{N}(\bar{\mathbf{x}})
\end{equation}
With this substitution in place, the derivation proceeds as follows
\begin{equation}
  \label{eqn:icp_update_derivation}
  \begin{split}
    % Line 1.
    \frac{\partial E}{\partial \mathbf{R}_{\{\alpha, \beta, \gamma\}}} & =
    \frac{\partial}{\partial \mathbf{R}_{\{\alpha, \beta, \gamma\}}}
    \sum_{p \in \mathbf{\Omega}}
    \norm{\phi(.)}\\
    % Line 2.
    & = \sum_{p \in \mathbf{\Omega}}
    \frac{\partial}{\partial \mathbf{R}_{\{\alpha, \beta, \gamma\}}}
    \norm{\phi(.)}\\
    % Line 3.
    & = \sum_{p \in \mathbf{\Omega}}
    \frac{\partial}{\partial \phi(.)} \norm{\phi(.)}
    \frac{\partial \phi(.)}{\partial \mathbf{R}}
    \frac{\partial \mathbf{R}}{\partial \{\alpha, \beta, \gamma\}}\\
    % Line 4.
    & = \sum_{p \in \mathbf{\Omega}}
    \frac{1}{2}\frac{2 \phi(.)}{\sqrt{\phi(.)^{T}\phi(.)}}
    \frac{\partial \phi(.)}{\partial \mathbf{R}}
    \frac{\partial \mathbf{R}}{\partial \{\alpha, \beta, \gamma\}}\\
    % Line 5.
    & = \sum_{p \in \mathbf{\Omega}}
    \frac{\phi(.)}{\norm{\phi(.)}}
    \frac{\partial \phi(.)}{\partial \mathbf{R}}
    \frac{\partial \mathbf{R}}{\partial \{\alpha, \beta, \gamma\}}\\
    % Line 6. - TODO: Check this last step.
    & = \sum_{p \in \mathbf{\Omega}}
    \frac{\phi(.)}{\norm{\phi(.)}}
    [\mathbf{x}^{T}\mathcal{N}(\bar{\mathbf{x}})]
    \frac{\partial \mathbf{R}}{\partial \{\alpha, \beta, \gamma\}}
  \end{split}
\end{equation}
Note that the derivation for the Partial Derivative
$\frac{\partial E}{\partial \mathbf{t}}$ is analogous and has been omitted.
Note that for the Translation Gradient, the term
$\frac{\partial \mathbf{R}}{\partial \{\alpha, \beta, \gamma\}}$ is replaced
with $\frac{\partial \mathbf{t}}{\partial \{t_{x}, t_{y}, t_{z}\}}$.

The full Partial Derivatives $\frac{\partial \mathbf{R}}{\partial \alpha}$,
$\frac{\partial \mathbf{R}}{\partial \beta}$ and
$\frac{\partial \mathbf{R}}{\partial \gamma}$ may be found Equations
\ref{eqn:rodrigues_full_alpha_deriv}, \ref{eqn:rodrigues_full_beta_deriv}
and \ref{eqn:rodrigues_full_gamma_deriv} of Appendix 
\ref{appendix:mathematical}.

The Jacobian of Equation \ref{eqn:pose_estimation_kf} is thus defined as
follows
\begin{equation}
  \label{eqn:pose_jacobian}
  \begin{split}
    \mathbf{J} & = \left. \frac{\partial E}{\partial
        \mathbf{R}_{\{\alpha, \beta, \gamma\}}, \mathbf{t}_{\{x, y, z\}}}
    \right\vert_{\alpha,\beta,\gamma = 0}\\
    & =
    \begin{bmatrix}
      0 & z & -y \\
      -z & 0 & x \\
      y & -x & 0 \\
      1 & 0 & 0 \\
      0 & 1 & 0 \\
      0 & 0 & 1
    \end{bmatrix}
  \end{split}
\end{equation}

\subsubsection{Pose Recovery Optimisation}
\label{subsub:moseg_static_camera_poserec_opt}

\section{Volumetric Fusion with Dynamic Scenes}
\label{sec:moseg_dynamic_fusion}

\section{Qualitative Results}
\label{sec:moseg_qualitative}

\section{Quantitative Results}
\label{sec:moseg_quantitative}

\section{Application to Semantic Scene Understanding}
\label{sec:moseg_semantic}