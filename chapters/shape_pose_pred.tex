\label{chap:spp}
\section{Introduction}
~\label{sec:spp_introduction}
In the Computer Vision literature, there has been research on the reconstruction of objects
from observed points in a range image, such as those obtained with an RGBD sensor such 
as the Microsoft Kinect. As outlined in Chapter~\ref{chap:probobj}, progress has been made 
on the techniques used, such that globally consistent models of an object of interest can be 
easily obtained.

Though the traditional reconstruction paradigm is suitable for tasks such as 3D data 
collection, it is less applicable in scenarios where full sensor coverage of the object 
of interest is not possible, as outlined in Section~\ref{sec:intro_aims_structure}. When a full 
view of an object is not available, only a partial reconstruction may be built. Data driven, 
learning based approaches that yield reconstructions as predictions from a generative model 
however do not have such a limitation. However, for application as a direct replacement for 
manual reconstruction, pose estimation must also be performed.

There has been much progress on learning based methods for both pose estimation and 
shape prediction, as outlined in Section~\ref{sec:lit_review_prediction}. However, much of 
this progress has been on the two problems when decoupled. In this work, an approach is 
proposed to the problem of simultaneous shape and pose \textit{prediction}. The first 
difference in this work to those outlined in Chapters~\ref{chap:moseg} and~\ref{chap:probobj} 
is the use of stereo RGB frames versus RGBD\@. 

The reason for the change of input data format is twofold. First, the use of RGBD is prohibitive 
when considering outdoor environments where lighting conditions may prevent RGBD sensors such as the 
Microsoft Kinect from producing a meaningful depth map. Second, the intuition that utilising multiple 
views of an object (from each sensor in the stereo pair) will make the pose estimation more amenable to 
learning based methods.

The second major deviation from the contributions of Chapters~\ref{chap:moseg} and~\ref{chap:probobj} 
is the removal of the dependence on input \textit{sequences}. Rather, in this work 
the proposed system performs shape and pose prediction from an \textit{instantaneous}
stereo pair, requiring no temporal consistency in both the training and prediction 
phases. As such, there is no iterative solving for shape and pose, rather the process 
is framed as an instananeous regression task. However, this work makes use of volumetric
shape representations, as with the other works outlined in Chapters~\ref{chap:moseg} and
~\ref{chap:probobj}.

The approach outlined in this chapter makes use of Siamese~\cite{SIAMESE} 
Convolutional Neural Networks and Gaussian Process Latent Variable Models to jointly regress 
shape and pose given a stereo pair with an object of interest present in both frames. The training of the 
model is unsupervised and requires only a detection or segmentation of the object of interest in the input 
frames to train. The proposed system regresses rotational and translational parameters directly 
from the neural network, whilst the object shape is a draw from a Gaussian Process 
Latent Variable Model. The use of a GPLVM for latent space embeddings of 3D shape 
allows for a simple way to learn complex 3D geometrical features, thus simplifying 
the learning tasks of the neural network.

The system is trained on an information theoretic loss over a rendering of a predicted 
shape under it's associated predicted pose, with respect to a given detection bounding box 
or semantic segmentation. 

\section{Algorithmic Overview}
~\label{sec:spp_algorithm}
The proposed model takes as input an RGB stereo pair with the object of interest within the 
view frustrum of both frames. From this stereo pair, a Convolutional Neural Network extracts 
feature descriptors which are used to regress a latent space point for shape, and six 
\( \mathbb{SE}(3) \) pose parameters, forming a Lie Algebra. The genrated shape for the proposal 
latent space point is drawn from the distribution of the GP prior conditioned on the latent space 
point. The form of the pose is the Lie Group mapping of the parameters, as in Sections
~\ref{subsub:moseg_static_camera_attitude} and~\ref{subsec:probobj_analytic_alignment_map}. 

The goodness of fit of the regressed shape and pose is quantified by by rendering the generated shape 
under the predicted pose and computing an information theoretic loss between the image statistics of 
the rendered region vs the initial detection region. To maintain continuity, the rendering operation is 
formulated in a differentiable manner. As there is often no ground truth pose and shape for ``real world'' 
data, training is performed in an unsupervised manner against a detection of the object os interest.

\subsection{Network Architecture}
~\label{subsec:spp_network_architecture}
The model of the proposed approach is of a siamese topology, where for a given stereo 
pair each of the left and right frames are passed to a separate network. From end to 
end, the model contains for each of the siamese networks a ResNet-50~\cite{He2015}, 
Linear Layers, Nonlinear Activations, a mapping from Lie Algebra to Lie Group (for pose),
a GPLVM followed by an Inverse Discrete Cosine Transform, a Raycaster and an information 
theoretic loss. 

The purpose of the ResNet-50 components is to extract pertinent features from the stereo 
pair that are descriptive of the the object of interest in relation to the scene. For each 
of the networks, the output of the ResNet-50 network is fed into two subnetworks consisting 
of linear transforms and nonlinear activations. The purpose of these subnetworks is to regress 
a latent space point for object shape, and a 6DoF pose parameter vector for the object pose. 
Following these two subnetworks is the aforementioned GPLVM and Lie Algebra mapping. The 
GPLVM generates a posterior mean over shape for a given latent space point, whilst the 
Lie Algebra mapping generates an \( \mathbb{SE}(3) \) transform. 

The IDCT decompresses the posterior mean output of the GPLVM to generate a valid SDF\@. Both the 
resultant SDF and \( \mathbb{SE}(3) \) transform are passed to the Raycasting module, which generates 
a rendering for the candidate shape and pose. Finally, this rendering with the initial detection is passed 
to the loss layer which computes appearance statistics for the detection region and the rendering 
which are used to quantify loss. The topology of the proposed model is outlined in Figure
~\ref{A}
FIGURE HERE

\subsection{Gaussian Process Latent Variable Model}
~\label{subsec:spp_gplvm}
The use of a GPLVM for the embedding of 3D shape is motivated by the assumption that for a 
given category of object, cars for example, there is enough shared geometric structure that 
a comprehensive generative model may be formed. In the linear case, the GPLVM is equivelant 
to a probabilistic formulation of Principal Component Analysis (PCA), where a linear mapping 
from observed to hdden, latent space is derived. 

Under a bayesian formulation, the often intractable latent variables pertaining to the linear 
mapping may be marginalised such that the latent embedding itself may be optimised directly. The 
given framework allows for complex, nonlinear embeddings to be learnt via the use kernel functions, 
though the optmisation in this case is often non-convex and highly nonlinear.

\subsubsection{Gaussian Process Marginal Likelihood}
~\label{subsubsec:spp_gp_marginal_likelihood}
Given a Latent Variable Model of the following form
\begin{equation}
  \label{eqn:spp_gp_lvm}
  P(\bm{Y} \given \bm{X}, \bm{W}, \beta) = P(\bm{Y} \given \bm{WX}^{T}, \beta^{-1}\bm{I})
\end{equation}
Where in Equation~\ref{eqn:spp_gp_lvm} the observed data \(\bm{Y} \in \mathbb{R}^{N \times D}\) 
is mapped to a lower dimensionality manifold \(\bm{X} \in \mathbb{R}^{N \times P}\), by parameters 
\(\bm{W} \in \mathbb{R}^{P \times D}\) and variance \( \beta \). In Equation~\ref{eqn:spp_gp_lvm}, 
the latent variable model \( P(\bm{Y} \given \bm{X}, \bm{W}, \beta) \) provides a linear mapping 
defined by the aforementioned latent variables \( \bm{W} \). As \( \bm{W} \) is unobservable and 
thus not directly tractable, it may be analytically marginalised out, given a suitable likelihood 
and prior.

The marginal likelihood of \(\bm{X}\) is of the form outlined in Equation~\ref{eqn:spp_gp_marginal}.
\begin{equation}
  \label{eqn:spp_gp_marginal}
  P(\bm{Y} \given \bm{X}, \beta) = \int P(\bm{Y} \given \bm{X}, \bm{W}, \beta) P(\bm{W}) \intd{\bm{W}}
\end{equation}
Where in Equation~\ref{eqn:spp_gp_marginal}, \( P(\bm{Y} \given \bm{X}, \bm{W}, \beta) \) is of 
a multivariate Guassian form and \(P(\bm{W})\) is a multivariate Gaussian conjugate prior of the 
form \(\mathcal{N}(\bm{W} \given \bm{0}, \bm{I})\).

To find the marginal distribution outlined in Equation~\ref{eqn:spp_gp_marginal}, it's form may 
first be simplified as follows in Equation~\ref{eqn:spp_gp_marginal_simplify}.

\begin{align}
  \label{eqn:spp_gp_marginal_simplify}
  % Line 1.
  P(\bm{Y} \given \bm{X}, \beta) ={}& \int \mathcal{N}(\bm{Y} \given \bm{WX}^{T}, \beta^{-1}\bm{I})
  \mathcal{N}(\bm{W} \given \bm{0}, \bm{I}) \intd{\bm{W}} \\
  % Line 2.
  ={}& \int \frac{1}{\sqrt{\left|2\pi\beta^{-1}\bm{I}\right|}} e^{ 
    -\frac{\beta}{2} {\big( \bm{Y} - \bm{WX} \big)}^{T} \big( \bm{Y} - \bm{WX} \big) 
  }
  \frac{1}{\sqrt{\left|2\pi\bm{I}\right|}} e^{ 
    -\frac{1}{2} \bm{W}^{T}\bm{W}} \intd{\bm{W}} \\
  % Line 3.
  ={}& \frac{1}{\sqrt{\left|2\pi\beta^{-1}\bm{I}\right|}} \frac{1}{\sqrt{\left|2\pi\bm{I}\right|}} 
  \int e^{
    -\frac{\beta}{2} {\big( \bm{Y} - \bm{WX} \big)}^{T} \big( \bm{Y} - \bm{WX} \big) 
    -\frac{1}{2} \bm{W}^{T}\bm{W}} \intd{\bm{W}} \\
  % Line 4.
  \propto& \int e^{
    -\frac{\beta}{2} {\big( \bm{Y} - \bm{WX} \big)}^{T} \big( \bm{Y} - \bm{WX} \big) 
    -\frac{1}{2} \bm{W}^{T}\bm{W}} \intd{\bm{W}} \\
  % Line 5.
  \propto& \int e^{ 
  -\frac{1}{2} \Big[
    \beta {\big( \bm{Y} - \bm{X}^{T}\bm{W} \big)}^{T} \big( \bm{Y} - \bm{X}^{T}\bm{W} \big) 
    + \bm{W}^{T}\bm{W}
  \Big]} \intd{\bm{W}} \\
  % Line 6.
  \propto& e^{ -\frac{\beta}{2} \bm{Y}^{T}\bm{Y}} 
  \int e^{
  -\frac{1}{2} \Big[
    -\beta \big( \bm{Y}^{T}\bm{X}^{T}\bm{W} \big) 
    -\beta {\big( \bm{W}^{T}\bm{XY} \big)}^{T}
    +\beta \bm{W}^{T}\bm{XX}^{T}\bm{W}
    + \bm{W}^{T}\bm{W}\Big]} \intd{\bm{W}} \\
  % Line 7.
  \propto& e^{-\frac{\beta}{2} \bm{Y}^{T}\bm{Y}} 
  \int e^{
  -\frac{1}{2} \Big[
    -2\beta \bm{Y}^{T}\bm{X}^{T}\bm{W}
    +\beta \bm{W}^{T}\bm{XX}^{T}\bm{W}
    + \bm{W}^{T}\bm{W}
  \Big]} \intd{\bm{W}} \\
  % Line 7.
  \propto& e^{-\frac{\beta}{2} \bm{Y}^{T}\bm{Y}} 
  \int e^{
  -\frac{1}{2} \Big[
    \bm{W}^{T} \big( \beta \bm{XX}^{T} + \bm{I} \big) \bm{W}
    -2\beta \bm{Y}^{T}\bm{X}^{T}\bm{W}
  \Big]} \intd{\bm{W}}
\end{align}

To make the integral over \(\bm{W}\) tractable in Equation~\ref{eqn:spp_gp_marginal_simplify}, 
the distribution \(P(\bm{Y} \given \bm{X}, \beta)\) must be a Gaussian; an exponential 
of a quadratic form. Completing the square in \(\bm{W}\) allows the marginal to be expressed 
in such a form. First, a change of variables is made, as in Equation~\ref{eqn:spp_gp_marginal_change}.
\begin{align}
  \label{eqn:spp_gp_marginal_change}
  \bm{A} ={}& \beta \bm{XX}^{T} + \bm{I}\\
  \bm{b} ={}& \beta \bm{Y}^{T} \bm{X}^{T}
\end{align}

The procedure to integrate over \( \bm{W} \) and transform \( P(\bm{Y} \given \bm{X}, \beta) \) 
into a valid multivariate Gaussian form is as follows in Equation~\ref{eqn:spp_gp_complete}.
\begin{align}
  \label{eqn:spp_gp_complete}
  % Line 1.
  P(\bm{Y} \given \bm{X}, \beta) \propto{}& e^{-\frac{\beta}{2}\bm{Y}^{T}\bm{Y}}
  \int e^{-\frac{1}{2} 
  \big[
    \bm{W}^{T}\bm{AW} - 2\bm{bW}  
  \big]} \intd{\bm{W}}\\
  % Line 2.
  \propto& e^{-\frac{\beta}{2}\bm{Y}^{T}\bm{Y}}
  \int e^{ 
    -\frac{1}{2} \bm{W}^{T}\bm{AW} 
    - 2\bm{bW} 
    - \bm{b}^{T}\bm{A}^{-1}\bm{b}} \intd{\bm{W}}\\
  % Line 3.
  \propto& e^{-\frac{\beta}{2}\bm{Y}^{T}\bm{Y}}
  \int e^{ 
    -\frac{1}{2} \bm{W}^{T}\bm{AW} 
    - 2\bm{bA}\bm{A}^{-1}\bm{W}
    + \bm{b}^{T}\bm{A}^{-1}\bm{AA}^{-1}\bm{b}} \intd{\bm{W}}\\
  % Line 4.
  \propto& e^{-\frac{\beta}{2}\bm{Y}^{T}\bm{Y}}
  \int e^{-\frac{1}{2} \big[ 
      {\big( \bm{W} - \bm{A}^{-1}\bm{b} \big)}^{T}
      \bm{A}
      \big( \bm{W} - \bm{A}^{-1}\bm{b} \big)
      - \bm{b}^{T}\bm{A}^{-1}\bm{b}
    \big]} \intd{\bm{W}}\\
  % Line 5.
  \propto& e^{-\frac{\beta}{2}\bm{Y}^{T}\bm{Y}}
  e^{-\frac{1}{2} \big[
      \sqrt{\left| 2 \pi \bm{A} \right|}
      - \bm{b}^{T}\bm{A}^{-1}\bm{b}
    \big]}\\
  % Line 6.
  \propto& e^{\frac{1}{2} \big[
    \beta \bm{Y}^{T} \beta \bm{Y}
    - \bm{b}^{T}\bm{A}^{-1}\bm{b}
    \big]}\\
  % Line 7.
  \propto& e^{-\frac{1}{2}
  \bm{Y}^{T} \big(
    \beta \bm{I} - \beta^{2}\bm{X}^{T}\bm{A}^{-1}\bm{X}
    \big)\bm{Y}}
\end{align}

With the distribution \(P(\bm{Y} \given \bm{X}, \beta)\) derived as being proportional to  
an exponentiated quadratic form as in Equation~\ref{eqn:spp_gp_complete}, it is 
clear that the inverse covariance matrix \(\bm{\Sigma}^{-1}\) of the Gaussian distribution 
corresponding to \(P(\bm{Y} \given \bm{X}, \beta)\) is as follows in Equation 
~\ref{eqn:spp_sig_inv}.
\begin{align}
  \label{eqn:spp_sig_inv}
  % Line 1.
  \bm{\Sigma}^{-1} ={}& \beta \bm{I} - \beta^{2} \bm{X}^{T} \bm{A}^{-1} \bm{X}\\
  % Line 2.
  ={}& \beta \bm{I} - \beta^{2} \bm{X}^{T} {\big(\beta \bm{XX}^{T} + \bm{I} \big)}^{-1}
\end{align}

To obtain the covariance matrix of the distribution \(P(\bm{Y} \given \bm{X}, \beta)\), the 
form of it's inverse may be simplified by the use of the Matrix Inversion Lemma (also known 
as the Woodbury Identity)~\cite{GPML}. First making a change of variables in the Woodbury Identity 
as in Equation~\ref{eqn:spp_sig_inv_simp}.
\begin{align}
  % Variable change.
  \label{eqn:spp_sig_inv_simp}
    \bm{A} ={}& \beta^{-1}\bm{I}\\
    \bm{C} ={}& \bm{I}\\
    \bm{U} ={}& \bm{X}^{T}\\
    \bm{V} ={}& \bm{X}
  \shortintertext{in}
  % Woodbury.
  (\bm{A} + \bm{UCV}) ={}&
  \bm{A}^{-1} - \bm{A}^{-1}\bm{U} 
  {(\bm{C}^{-1} + \bm{VA}^{-1}\bm{U})}^{-1}
  \bm{VA}^{-1}
\end{align}

The simplified form of \( \bm{\Sigma}^{-1} \) is thus given as follows in 
Equation~\ref{eqn:spp_sig_inv_simp}.
\begin{align}
  \label{eqn:spp_sig_inv_simp}
  % Line 1.
  \bm{\Sigma}^{-1} ={}& \beta \bm{I} - \beta^{2} \bm{X}^{T} 
  {\big(\beta \bm{XX}^{T} + \bm{I} \big)}^{-1}\\
  % Line 2.
  ={}& \beta^{-1} \bm{I} + \bm{X}^{T}\bm{X}
\end{align}

It follows from Equation~\ref{eqn:spp_sig_inv_simp}, that the covariance matrix \( \bm{\Sigma} \)
takes the following form.
\begin{align}
  \label{eqn:spp_sig}
  % Line 1.
  \bm{\Sigma} ={}& {(\bm{\Sigma}^{-1})}^{-1}\\
  % Line 2.
  ={}& \bm{X}^{T}\bm{X} + \beta^{-1}\bm{I}
\end{align}

As such, the form of the normalized marginal likelihood \(P(\bm{Y} \given \bm{X}, \beta)\) of 
\( \bm{W} \) is given in Equation~\ref{eqn:spp_gp_marginal_simp}.
\begin{align}
  \label{eqn:spp_gp_marginal_simp}
  % Line 1.
  P(\bf{Y \given \bm{X}, \beta}) ={}& \mathcal{N}(\bm{Y} \given \bm{0}, 
  \bm{X}^{T}\bm{X} + \beta^{-1}\bm{I})\\
  % Line 2.
  ={}& TODO
\end{align}

\subsubsection{Gaussian Process Fitting}
~\label{subsubsec:spp_gp_fitting}
As outlined in Section~\ref{subsec:spp_gplvm}, the given formulation of the marginal likelihood 
in Equation~\ref{eqn:spp_gp_marginal_simp} can be optimised directly for the latent embedding 
\( \bm{X} \). For a mapping from \(\mathbb{R}^{N \times D} \) space to \(\mathbb{R}^{N \times P} \)
space (for \( P < D \)), the latent embedding \( \bm{X} \) may be initialized by applying an 
orthogonal linear transform to the observed data and reducing dimensionality. One such approach 
is to apply Principal Component Analysis (PCA) to the observed data, taking the first \(P\) 
reverse sorted Eigenvalues of the covariance matrix.

Additionally, in the nonlinear case, where the covariance matrix \( \bm{\Sigma} \) is generated by 
a given kernel function \( \bm{\kappa} \), the hyperparameters of \( \bm{\kappa} \) may also be 
optimised.

To find the most probable latent space embedding, the latent variables \( \bm{X} \) may be 
found by directly optimising the marginal likelihood of Equation~\ref{eqn:spp_gp_marginal_simp}. 
As such, the natural logarithm may also be optimised for the latent variables \( \bm{X} \) and 
is given in Equation~\ref{eqn:spp_gp_marginal_log}.
\begin{align}
  \label{eqn:spp_gp_marginal_log}
  % Line 1.
  \mathcal{L} ={}& -\frac{DN}{2} \ln(2\pi)
  -\frac{D}{2} \ln(\left| \bm{\Sigma} \right|)
  -\frac{1}{2} \text{tr}(\bm{\Sigma}^{-1} \bm{YY}^{T})\\
  % Line 2.
  ={}& -\frac{DN}{2} \ln(2\pi)
  -\frac{D}{2} \ln(\left| \bm{\Sigma} \right|)
  -\frac{1}{2} \bm{Y}^{T}\bm{\Sigma}\bm{Y}
\end{align}

The gradient of the log marginal of Equation~\ref{eqn:spp_gp_marginal_log} 
is derived as follows in Equation~\ref{eqn:spp_gp_log_marginal_grad}.
\begin{align}
  \label{eqn:spp_gp_log_marginal_grad}
  % Line 1.
  \frac{\partial \mathcal{L}}{\partial \bm{X}} ={}&
  -\frac{1}{2} \Bigg[
    \Big( D \frac{\partial}{\partial \bm{\Sigma}} 
    \ln \big( \left| \bm{\Sigma} \right| \big) \Big) 
    \frac{\partial \bm{\Sigma}}{\partial \bm{X}}
    + \Big( \frac{\partial}{\partial \bm{\Sigma}}
    \bm{Y}^{T} \bm{\Sigma}^{-1} \bm{Y} \Big)
    \frac{\partial \bm{\Sigma}}{\partial \bm{X}}
  \Bigg]\\
  % Line 2.
  ={}& -\frac{1}{2} \Bigg[
    D \bm{\Sigma}^{-1} \frac{\partial \bm{\Sigma}}{\partial \bm{X}}
    - \bm{\Sigma}^{-1} \bm{YY}^{T} \bm{\Sigma}^{-1} 
    \frac{\partial \bm{\Sigma}}{\partial \bm{X}}
  \Bigg]\\
  % Line 3.
  ={}& -\frac{1}{2} \Bigg[
    D \bm{\Sigma}^{-1} 2 \bm{X}
    - \bm{\Sigma}^{-1} \bm{YY}^{T} \bm{\Sigma}^{-1} 2 \bm{X}
  \Bigg]\\
  % Line 4.
  ={}& -D \bm{\Sigma}^{-1} \bm{X}
  + \bm{\Sigma}^{-1} \bm{YY}^{T} \bm{\Sigma}^{-1} \bm{X}
\end{align}

The gradient derived in Equation~\ref{eqn:spp_gp_log_marginal_grad} holds 
for the case when \( \bm{\Sigma} = \bm{X}^{T}\bm{X} + \beta^{-1} \bm{I} \). 
However, for \( \bm{\Sigma} = \bm{\kappa}(.) \) where \( \bm{\kappa} \) is 
a given kernel function, the result of the derivation of Equation
~\ref{eqn:spp_gp_log_marginal_grad} is applicable. When subsituting 
\( \frac{\partial \bm{\Sigma}}{\partial \bm{X}} \) with 
\( \frac{\partial \bm{\kappa}}{\partial \bm{X}} \), the gradient is thus 
given in Equation~\ref{eqn:spp_gp_log_marginal_grad_kernel}.
\begin{equation}
  \label{eqn:spp_gp_log_marginal_grad_kernel}
  \frac{\partial \mathcal{L}}{\partial \bm{X}} = 
  -\frac{1}{2} \Bigg[
    D \bm{\Sigma}^{-1} \frac{\partial \bm{\kappa}}{\partial \bm{X}}
    - \bm{\Sigma}^{-1} \bm{YY}^{T} \bm{\Sigma}^{-1} 
    \frac{\partial \bm{\kappa}}{\partial \bm{X}}
  \Bigg]
\end{equation}

It should be noted that the gradient \( \frac{\partial \bm{\kappa}}{\partial \bm{X}} \)
may also be substitued for \( \frac{\partial \bm{\kappa}}{\partial \theta} \), for 
some hyperparameter \( \theta \) of the kernel \( \bm{\kappa} \).

A common nonlinear covariance kernel function in the Gaussian Process literature 
is the Exponentiated Quadratic~\cite{Lawrence2005}, which takes the form given in 
Equation~\ref{eqn:spp_exp_quad}.
\begin{align}
  \label{eqn:spp_exp_quad}
  % Line 1.
  \bm{\kappa} \big( \bm{x}_{i}, \bm{x}_{j}, \theta_{0}, 
  \theta_{1}, \theta_{2}, \lambda \big) ={}&
  \theta_{0} e^{-\frac{\lambda}{2} 
  \left\lVert \bm{x}_{i} - \bm{x}_{j} \right\rVert^{2}}
  + \theta_{1} + \theta_{2} \delta \big( -\frac{\lambda}{2} 
  \left\lVert \bm{x}_{i} - \bm{x}_{j} \right\rVert^{2} \big)\\
  % Line 2.
  ={}& \theta_{0} e^{-\frac{\lambda}{2} 
  \sum_{n}^{D} {\big( \bm{x}_{i, n} - \bm{x}_{j, n} \big)}^{2}}
  + \theta_{1} + \theta_{2} \delta \big( -\frac{\lambda}{2} 
  \sum_{n}^{D} {\big( \bm{x}_{i, n} - \bm{x}_{j, n} \big)}^{2} \big)
\end{align}

The gradient of the Exponentiated Quadratic kernel of Equation~\ref{eqn:spp_exp_quad},
\( \frac{\partial \bm{\kappa}}{\partial \bm{x}_{i, n}} \) for the \( n^{th} \) variable 
of \( \bm{x}_{i} \) can be derived as follows in Equation~\ref{eqn:exp_quad_grad_x}.
\begin{align}
  \label{eqn:exp_quad_grad_x}
  % Line 1.
  \frac{\partial \bm{\kappa}}{\partial \bm{x}_{i, n}} ={}& 
  \frac{\partial}{\partial \bm{x}_{i, n}} \theta_{0} e^{-\frac{\lambda}{2} 
  \sum_{n = 0}^{D} -\frac{\lambda}{2} {\big( \bm{x}_{i, n} - \bm{x}_{j, n} \big)}^{2}}\\
  % Line 2.
  ={}& \frac{\partial}{\partial \bm{x}_{i, n}} \theta_{0} e^{-\frac{\lambda}{2} 
  \sum_{n = 0}^{D} {\big( \bm{x}_{i, n} - \bm{x}_{j, n} \big)}^{2}} 
  \frac{\partial}{\partial \bm{x}_{i, n}} \sum_{n = 0}^{D} -\frac{\lambda}{2} 
  {\big( \bm{x}_{i, n} - \bm{x}_{j, n} \big)}^{2}\\
  % Line 3.
  ={}& \theta_{0} e^{-\frac{\lambda}{2} 
  \sum_{n = 0}^{D} {\big( \bm{x}_{i, n} - \bm{x}_{j, n} \big)}^{2}} 
  \frac{\partial}{\partial \bm{x}_{i, n}} \sum_{n = 0}^{D} -\frac{\lambda}{2} 
  {\big( \bm{x}_{i, n} - \bm{x}_{j, n} \big)}^{2}\\
  % Line 4.
  ={}& \lambda \theta_{0} e^{-\frac{\lambda}{2} 
  \sum_{n = 0}^{D} {\big( \bm{x}_{i, n} - \bm{x}_{j, n} \big)}^{2}} 
  {\big( \bm{x}_{i, n} - \bm{x}_{j, n} \big)}
\end{align}

Following the derivation of Equation~\ref{eqn:exp_quad_grad_x}, the remaining 
gradients of \( \kappa(.) \) may be trivially derived, as in Equation
~\ref{eqn:exp_quad_grad_rest}.
\begin{align}
  \label{eqn:exp_quad_grad_rest}
  % Line 1.
  \frac{\partial \bm{\kappa}}{\partial \bm{x}_{j, n}} ={}& 
  -\frac{\partial \bm{\kappa}}{\partial \bm{x}_{i, n}}\\
  % Line 2.
  \frac{\partial \bm{\kappa}}{\partial \theta_{0}} ={}&
  e^{-\frac{\lambda}{2} 
  \sum_{n = 0}^{D} -\frac{\lambda}{2} {\big( \bm{x}_{i, n} - \bm{x}_{j, n} \big)}^{2}}\\
  % Line 3.
  \frac{\partial \bm{\kappa}}{\partial \theta_{1}} ={}& 1\\
  % Line 4.
  \frac{\partial \bm{\kappa}}{\partial \theta_{2}} ={}& 0\\
  % Line 5.
  \frac{\partial \bm{\kappa}}{\partial \lambda} ={}& \text{TODO}
\end{align}

\subsection{Latent Space Shape Estimation}
~\label{sub:spp_latent_shape_est}
The form of the model outlined in Section~\ref{subsubsec:spp_gp_marginal_likelihood}
when trained as outlined in Section~\ref{subsubsec:spp_gp_fitting} defines a Gaussian 
Process prior over the latent space embedding \( \bm{X} \). When regressing shape, it 
is desirable to predict the 3D shape of a given latent space point, conditioned on this 
prior. This conditioning yields a posterior mean estimation over 3D shape for a given 
latent space point.

The estimated posterior mean provides a DCT compressed representation of an SDF shape 
volume \( \bm{\Phi} \in \mathbb{R}^{N \times N \times N} \). The DCT compressed form 
of \( \bm{\Phi} \) given by the aforementioned posterior mean is obtained by Gaussian 
Process Regression~\cite{GPML}. Finally, the true, uncompressed form of \( \bm{\Phi} \) is 
obtained by taking the IDCT of the posterior mean. The granularity of the geometric 
properties captured by the GP shape prior is governed by the number of DCT harmonics 
used in the compression and decompression processes at training and prediction.

\subsubsection{Shape Posterior Mean Estimation}
~\label{subsubsec:spp_pos_mean_est}
With the optimised latent variables \( \bm{X} \), the formulation outlined in 
Equation~\ref{eqn:spp_gp_marginal_simp} defines a Gaussian Process prior 
over functions of \( \bm{X} \), as follows.
\begin{equation}
  \label{eqn:gp_prior}
  \bm{f}(\bm{X}) \sim \mathcal{GP}(\bm{0}, \bm{\Sigma}_{\bm{XX}})
\end{equation}

A Gaussian Process prior may similarly be constructed for observed latent 
space points \( \bm{L} \), as follows.
\begin{equation}
  \label{eqn:gp_prior_latent}
  \bm{f}^{\star}(\bm{l}) \sim \mathcal{GP}(\bm{0}, \bm{\Sigma}_{\bm{LL}})
\end{equation}

It follows from Equations~\ref{eqn:gp_prior} and~\ref{eqn:gp_prior_latent} that 
the joint distribution over \( \bm{X} \) and \( \bm{L} \) can be formulated as 
follows in Equation~\ref{eqn:spp_gp_joint}.
\begin{equation}
  \label{eqn:spp_gp_joint}
  \begin{bmatrix}
    \bm{f}\\
    \bm{f}^{\star}
  \end{bmatrix}
  \sim \mathcal{N} \Bigg(
    \begin{bmatrix}
      \bm{0}\\
      \bm{0}
    \end{bmatrix},
    \begin{bmatrix}
      \bm{\Sigma}_{\bm{XX}} & \bm{\Sigma}_{\bm{XL}}\\
      \bm{\Sigma}_{\bm{LX}} & \bm{\Sigma}_{\bm{LL}}
    \end{bmatrix}
  \Bigg)
\end{equation}

The posterior of \( \bm{f}^{\star} \) conditioned on \( \bm{f} \) is given by
the distribution in Equation~\ref{eqn:spp_gp_conditional}.
\begin{equation}
  \label{eqn:spp_gp_conditional}
  P(\bm{f}^{\star} \given \bm{X}, \bm{L}, \bm{f}) = 
  \mathcal{N} \Big(
    \bm{\Sigma}_{LX} \bm{\Sigma}_{XX}^{-1} \bm{f}, 
    \bm{\Sigma}_{LL} - \bm{\Sigma}_{LX} \bm{\Sigma}_{XX}^{-1} \bm{\Sigma}_{XL}
  \Big) 
\end{equation}

As such, to obtain a posterior mean prediction for a latent space point \( \bm{l} \), 
a draw from the distribution outlined in Equation~\ref{eqn:spp_gp_conditional} is 
taken as follows.
\begin{equation}
  \label{eqn:spp_gp_posterior_draw}
  \bm{f}^{\star} \sim P(\bm{f}^{\star} \given \bm{X}, \bm{L}, \bm{f})
\end{equation}

Given the form of \( P(\bm{f}^{\star} \given \bm{X}, \bm{L}, \bm{f}) \) in Equation
~\ref{eqn:spp_gp_posterior_draw}, the posterior mean \( \bm{f}^{\star} \) and variance
\( \mathbb{V}^{\star} \) are given as follows in Equation~\ref{eqn:spp_mean_var}.
\begin{align}
  \label{eqn:spp_mean_var}
  % Line 1.
  \bm{f}^{\star} ={}& 
  \bm{\Sigma}_{\bm{XL}}^{T} \bm{\Sigma}_{\bm{XX}}^{-1} \bm{Y}\\
  % Line 2.
  \mathbb{\bm{f}^{\star}} ={}&
  \bm{\Sigma}_{\bm{LL}} - \bm{\Sigma}_{\bm{LL}}^{T} \bm{\Sigma}_{XL}^{-1} \bm{\Sigma}_{LL}
\end{align}

Due to the gradient based optimisation of the approach outlined in this chapter, each 
component outlined in Figure~\ref{A} must be differentiable. The formulation of the 
posterior mean of Equation~\ref{eqn:spp_mean_var} is differentiable and it's gradient 
is derived as follows in Equation~\ref{eqn:spp_pos_mean_grad}.
\begin{align}
  \label{eqn:spp_pos_mean_grad}
  % Line 1.
  \frac{\partial \bm{f}^{\star}}{\partial \bm{L}} ={}&
  \frac{\partial}{\partial \bm{L}}
  \bm{\Sigma}_{\bm{XL}}^{T} \bm{\Sigma}_{\bm{XX}}^{-1} \bm{Y}\\
  % Line 2.
  ={}& \frac{\partial \bm{\Sigma}_{\bm{XL}}^{T}}{\partial \bm{L}}
  \bm{\Sigma}_{\bm{XX}}^{-1} \bm{Y}
  + \bm{\Sigma}_{\bm{XL}}^{T} 
  \frac{\partial\bm{\Sigma}_{\bm{XX}}^{-1}}{\partial \bm{L}} \bm{Y}
  + \bm{\Sigma}_{\bm{XL}}^{T} \bm{\Sigma}_{\bm{XX}}^{-1} 
  \frac{\partial \bm{Y}}{\partial \bm{L}}\\
  % Line 3.
  ={}& \frac{\partial \bm{\Sigma}_{\bm{XL}}^{T}}{\partial \bm{L}}
  \bm{\Sigma}_{\bm{XX}}^{-1} \bm{Y}
\end{align}

\subsubsection{Signed Distance Function Extraction}
~\label{subsubsec:sdf_extraction}
As outlined in Section~\ref{sub:spp_latent_shape_est}, the SDF volume \( \bm{\Phi} \) 
is generated by taking the IDCT of the posterior mean of the GP prior conditioned on 
a given latent space point, as given by Equation~\ref{eqn:spp_gp_conditional}. In the 
formulation presented in this work, the 2D latent point is generated by the pose 
network component following the Resnet-50 CNN of Figure~\ref{A}. The output of the network 
is constrained to the interval \( [0, 1] \) by the use of a Sigmoid activation function.

The latent variable model outlined in Section~\ref{subsec:spp_gplvm} is a distribution 
over a latent space embedding of 3D shape. However, the 3D shape data on which the GP 
prior was modelled is compressed with the DCT\@. The intuition behind the use of the DCT 
is the property that the number of harmonics used in the transform impacts on the 
granularity of the resultant 3D shape geometry. It has been shown~\cite{Ren2014} that the 
lower frequency harmonics of the DCT capture general structure and shape, whilst higher 
frequency harmonics capture finer detailed geometric features.

The voxels of an input SDF \( \bm{V} \) under the DCT (for model training) are thus given 
as follows in Equation~\ref{eqn:spp_dct}.
\begin{align}
  \label{eqn:spp_dct}
  % Line 1.
  \bm{\Psi}_{x, y, z} ={}& \bm{V}_{x, y, z} \Bigg[
  \sum_{x=0}^{N-1} \cos \Big[ \frac{\pi}{N} \big[ x + \frac{1}{2} \big] x \Big]
  \sum_{y=0}^{N-1} \cos \Big[ \frac{\pi}{N} \big[ y + \frac{1}{2} \big] y \Big]
  \sum_{z=0}^{N-1} \cos \Big[ \frac{\pi}{N} \big[ z + \frac{1}{2} \big] z \Big] \Bigg]\\
  % Line 2.
  ={}& \bm{V}_{x, y, z} \Bigg[
  \sum_{x=0}^{N-1} \cos \Big[ \frac{\pi \big( x^{2} + \frac{x}{2} \big)}{N} \Big]
  \sum_{y=0}^{N-1} \cos \Big[ \frac{\pi \big( y^{2} + \frac{y}{2} \big)}{N} \Big]
  \sum_{z=0}^{N-1} \cos \Big[ \frac{\pi \big( z^{2} + \frac{z}{2} \big)}{N} \Big] \Bigg]\\
  % Line 3.
  ={}& \sum_{x=0}^{N-1} \sum_{y=0}^{N-1} \sum_{z=0}^{N-1} \bm{V}_{x, y, z}
  \cos \Big[ \frac{\pi \big( x^{2} + \frac{x}{2} \big)}{N} \Big]
  \cos \Big[ \frac{\pi \big( y^{2} + \frac{y}{2} \big)}{N} \Big]
  \cos \Big[ \frac{\pi \big( z^{2} + \frac{z}{2} \big)}{N} \Big]
\end{align}

It follows that for a predicted posterior mean \( \bm{f}^{\star} \) as outlined in 
Equation~\ref{eqn:spp_gp_posterior_draw}, the voxels of the SDF corresponding to the 
estimation \( \bm{f}^{\star} \) may be extracted via the IDCT as follows in 
Equation~\ref{eqn:spp_idct}.
\begin{equation}
  \label{eqn:spp_idct}
  \bm{\Phi}_{x, y, z} = \sum_{x=0}^{N-1} \sum_{y=0}^{N-1} \sum_{z=0}^{N-1} 
  \bm{f}^{\star}_{x, y, z}
  \cos \Big[ \frac{\pi \big( x^{2} + \frac{x}{2} \big)}{N} \Big]
  \cos \Big[ \frac{\pi \big( y^{2} + \frac{y}{2} \big)}{N} \Big]
  \cos \Big[ \frac{\pi \big( z^{2} + \frac{z}{2} \big)}{N} \Big]
\end{equation}

The SDF \( \bm{\Phi} \) generated by the IDCT given in Equation~\ref{eqn:spp_idct} must be 
differentiable with respect to the posterior mean \( \bm{f}^{\star} \) for backpropagation 
training of the model. The gradent of the SDF \( \bm{\Phi} \) outlined in 
Equation~\ref{eqn:spp_idct} is derived as follows in Equation~\ref{eqn:spp_idct_grad}. 
For notational clarity, the DCT coefficients are defined as follows in Equation
~\ref{eqn:spp_dct_coeff}
\begin{equation}
  \label{eqn:spp_dct_coeff}
  \zeta(x, y, z) = 
  \cos \Big[ \frac{\pi \big( x^{2} + \frac{x}{2} \big)}{N} \Big]
  \cos \Big[ \frac{\pi \big( y^{2} + \frac{y}{2} \big)}{N} \Big]
  \cos \Big[ \frac{\pi \big( z^{2} + \frac{z}{2} \big)}{N} \Big]
\end{equation}
\begin{align}
  \label{eqn:spp_idct_grad}
  % Line 1.
  \frac{\partial \bm{\Phi}}{\partial \bm{f}_{x, y, z}^{\star}} ={}&
  \frac{\partial}{\partial \bm{f}_{x, y, z}^{\star}}
  \sum_{x=0}^{N-1} \sum_{y=0}^{N-1} \sum_{z=0}^{N-1} 
  \bm{f}^{\star}_{x, y, z} \zeta(x, y, z)\\
  % Line 2.
  ={}& \sum_{x=0}^{N-1} \sum_{y=0}^{N-1} \sum_{z=0}^{N-1} 
  \frac{\partial}{\partial \bm{f}_{x, y, z}^{\star}} \bm{f}^{\star}_{x, y, z}
  \zeta(x, y, z)\\
  % Line 3.
  ={}& \sum_{x=0}^{N-1} \sum_{y=0}^{N-1} \sum_{z=0}^{N-1} \Bigg[
    \Bigg[ 
      \frac{\partial}{\partial \bm{f}_{x, y, z}^{\star}} \bm{f}^{\star}_{x, y, z} 
    \Bigg] \zeta(x, y, z)
    + \bm{f}_{x, y, z}^{\star} \frac{\partial \zeta}{\partial \bm{f}_{x, y, z}^{\star}}
  \Bigg]\\
  % Line 4.
  ={}& \sum_{x=0}^{N-1} \sum_{y=0}^{N-1} \sum_{z=0}^{N-1}
  \nabla \bm{f}_{x, y, z}^{\star} \zeta(x, y, z)\\
  % Line 5.
  ={}& \sum_{x=0}^{N-1} \sum_{y=0}^{N-1} \sum_{z=0}^{N-1} 
  \nabla \bm{f}^{\star}_{x, y, z}
  \cos \Big[ \frac{\pi \big( x^{2} + \frac{x}{2} \big)}{N} \Big]
  \cos \Big[ \frac{\pi \big( y^{2} + \frac{y}{2} \big)}{N} \Big]
  \cos \Big[ \frac{\pi \big( z^{2} + \frac{z}{2} \big)}{N} \Big]
\end{align}
From Equation~\ref{eqn:spp_idct_grad} it is evident that the partial derivative 
\( \frac{\partial \bm{\Phi}}{\partial \bm{f}_{x, y, z}^{\star}} \) is trivial 
to compute as the derivative of the IDCT is simply the IDCT of the derivative. 
Furthermore, the gradient of the posterior mean is trivial to compute, as shown 
in Equation~\ref{eqn:spp_pos_mean_grad}.

\subsection{Pose Estimation}
~\label{subsec:spp_pose_estim}
The parameters of the \( \mathbb{SE}(3) \) pose applied to the predicted shape (obtained 
as outlined in Section~\ref{subsubsec:sdf_extraction}) are obtained from a fully connected 
component of the model outlined in Figure~\ref{A}, as with the latent pose point of Section
~\ref{subsubsec:sdf_extraction}. The form of the \( \mathbb{SE}(3) \) pose is analogous to 
the Rodrigues paramaterisation outlined in Section~\ref{subsub:moseg_static_camera_poserec}.

The three rotational parameters \( \alpha \), \( \beta \) and \( \gamma \) are predicted by 
the pose network component of the model on the interval \( [0, 2\pi) \). The translation parameters 
however do not have their output restricted. It should be noted that the pose Jacobians are of the 
same form of those outlined in Equations~\ref{eqn:rot_jac} and~\ref{eqn:trans_jac}.

\subsection{Rendering}
~\label{subsec:rendering}
% refer to prev(still to write) raycast
% differentiable by...
% refer back to log sigmoid and derivative
% give pseudocode
% example probability map

\subsection{Loss Function and Backpropagation}
~\label{subsec:spp_loss}

\section{Qualitative Results}
~\label{sec:spp_qualitative}

\section{Quantitative Results}
~\label{sec:spp_quantitative}

\section{Discussion}
~\label{sec:spp_discussion}