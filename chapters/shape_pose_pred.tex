\section{Introduction}
~\label{sec:spp_introduction}
In the Computer Vision literature, there has been work on the reconstruction of objects
from observed points in a range image, such as those obtained with an RGBD sensor such 
as the Microsoft Kinect. As outlined in Chapter~\ref{}, progress has been made on the 
techniques used, such that globally consistent models of an object of interest can be 
easily obtained.

Though the traditional reconstruction paradigm is suitable for tasks such as 3D data 
collection, it is less applicable in scenarios where full sensor coverage of the object 
of interest is not possible. When a full view of an object is not available, only a 
partial reconstruction may be built. Data driven, learning based approaches that yield 
reconstructions as predictions from a generative model however do not have such a 
limitation. However, for application as a direct replacement for manual reconstruction, 
pose estimation must also be performed.

There has been much progress on learning based methods for both pose estimation and 
shape prediction, as outlined in Section~\ref{}. However, much of this progress has 
been on the two problems when decoupled. In this work, an approach is proposed to the 
problem of simultaneous shape and pose \textit{prediction}. The first difference in 
this work to those outlined in Chapters~\ref{} and~\ref{} is the use of stereo RGB 
frames versus RGBD. The reason for the change in input data format is twofold. Firstly, 
the use of RGBD is prohibitive when considering outdoor environments where lighting 
conditions may prevent RGBD sensors such as the Microsoft Kinect from producing a 
meaningful depth map. Second, is the intuition that utilising multiple views of an 
object (from each sensor in the stereo pair) will make the pose estimation more 
amenable to learning based methods.

The second major deviation from the contributions of Chapters~\ref{} and~\ref{} 
is the removal of the dependence on input \textit{sequences}. Rather, in this work 
the proposed system performs shape and pose prediction from an \textit{instantaneous}
stereo pair, requiring no temporal consistency in both the training and prediction 
phases. As such, there is no iterative solving for shape and pose, rather the process 
is framed as an instananeous regression task. However, this work makes use of volumetric
shape representations, as with the other works outlined in this thesis.

The approach outlined in this chapter makes use of Siamese Convolutional Neural 
Networks and Gaussian Process Latent Variable Models for to jointly regress shape 
and pose given a stereo pair. The training of the model is unsupervised and requires 
only a detection or segmentation of the object of interest in the input frames to train. 
The proposed system regresses rotational and translational parameters directly 
from the neural network, whilst the object shape is a draw from a Gaussian Process 
Latent Variable Model. The use of a GPLVM for latent space embeddings of 3D shape 
allows for a simple way to learn complex 3D geometrical features, thus simplifying 
the learning tasks of the neural network.

The system is trained on an information theoretic loss over a rendering of a predicted 
shape under it's associated predicted pose, with respect to a given detection bounding box 
or semantic segmentation. 

\section{Related Work}
~\label{sec:spp_related}

\section{Algorithmic Overview}
~\label{sec:spp_algorithm}
The proposed model takes as input an RGB stereo pair with the object of interest within the 
view frustrum of both frames. From this stereo pair, a Convolutional Neural Network extracts 
feature descriptors which are used to regress a latent space point for shape, and six 
\( \mathbb{SE}(3) \) pose parameters, forming a Lie Algebra. The genrated shape for the proposal 
latent space point is drawn from the distribution of the GP prior conditioned on the latent space 
point. The form of the pose is the Lie Group mapping of the parameters, as in Sections~\ref{} 
and~\ref{}. The goodness of fit of the regressed shape and pose is quantified by by rendering the 
generated shape under the generated pose and computing an information theoretic loss between the 
image statistics of the rendered region vs the initial detection region. To maintain continuity, 
the rendering operation is formulated in a differentiable manner. As there is often no ground truth 
pose and shape for ``real world'' data, training is performed in an unsupervised manner against a 
detection of the object os interest.

\subsection{Network Architecture}
~\label{subsec:spp_network_architecture}
The model of the proposed approach is of a siamese topology, where for a given stereo 
pair each of the left and right frames are passed to a separate network. From end to 
end, the model contains for each of the siamese networks a ResNet-50~\cite{RESNET}, 
Linear Layers, Nonlinear Activations, a mapping from Lie Algebra to Lie Group (for pose),
a GPLVM followed by an Inverse Discrete Cosine Transform, a Raycaster and an information 
theoretic loss. 

The purpose of the ResNet-50 components is to extract pertinent features from the stereo 
pair that are descriptive of the the object of interest in relation to the scene. For each 
of the networks, the output of the ResNet-50 network is fed into two subnetworks consisting 
of linear transforms and nonlinear activations. The purpose of these subnetworks is to regress 
a latent space point for object shape, and a 6DoF pose parameter vector for the object pose. 
Following these two subnetworks is the aforementioned GPLVM and Lie Algebra mapping. The 
GPLVM generates a posterior mean over shape for a given latent space point, whilst the 
Lie Algebra mapping generates an \( \mathbb{SE}(3) \) transform. The IDCT decompresses 
the posterior mean output of the GPLVM to generate a valid SDF. Both the resultant SDF and 
\( \mathbb{SE}(3) \) transform are passed to the Raycasting module, which generates a rendering 
for the candidate shape and pose. Finally, this rendering with the initial detection is passed 
to the loss layer which computes appearance statistics for the detection region and the rendering 
which are used to quantify loss. The topology of the proposed model is outlined in Figure
~\ref{}
FIGURE HERE

\subsection{Gaussian Process Latent Variable Model}
~\label{subsec:spp_gplvm}
The use of a GPLVM for the embedding of 3D shape is motivated by the assumption that for a 
given category of object, cars for example, there is enough shared geometric structure that 
a comprehensive generative model may be formed. In the linear case, the GPLVM is equivelant 
to a probabilistic formulation of Principal Component Analysis (PCA), where a linear mapping 
from observed to hdden, latent space is derived. Under a bayesian formulation, the often 
intractable latent variables pertaining to the linear mapping may be marginalised such that 
the latent embedding itself may be optimised directly. The given framework allows for complex, 
nonlinear embeddings to be learnt via the use kernel functions, though the optmisation in this 
case is often non-convex and highly nonlinear.

\subsubsection{Gaussian Process Marginal Likelihood}
~\label{subsubsec:spp_gp_marginal_likelihood}
Given a Latent Variable Model of the following form
\begin{equation}
  \label{eqn:spp_gp_lvm}
  P(\bm{Y} \given \bm{X}, \bm{W}, \beta) = P(\bm{Y} \given \bm{WX}^{T}, \beta^{-1}\bm{I})
\end{equation}
Where in Equation~\ref{eqn:spp_gp_lvm} the observed data \(\bm{Y} \in \mathbb{R}^{N \times D}\) 
is mapped to a lower dimensionality manifold \(\bm{X} \in \mathbb{R}^{N \times P}\), by parameters 
\(\bm{W} \in \mathbb{R}^{P \times D}\) and variance \( \beta \). In Equation~\ref{eqn:spp_gp_lvm}, 
the latent variable model \( P(\bm{Y} \given \bm{X}, \bm{W}, \beta) \) provides a linear mapping 
defined by the aforementioned latent variables \( \bm{W} \). As \( \bm{W} \) is unobservable and 
thus not directly tractable, it may be integrated over, given a suitable likelihood and prior.

The marginal likelihood of \(\bm{X}\) is of the form outlined in Equation~\ref{eqn:spp_gp_marginal}.
\begin{equation}
  \label{eqn:spp_gp_marginal}
  P(\bm{Y} \given \bm{X}, \beta) = \int P(\bm{Y} \given \bm{X}, \bm{W}, \beta) P(\bm{W}) \intd{\bm{W}}
\end{equation}
Where in Equation~\ref{eqn:spp_gp_marginal}, \( P(\bm{Y} \given \bm{X}, \bm{W}, \beta) \) is of 
a multivariate Guassian form and \(P(\bm{W})\) is a multivariate Gaussian conjugate prior of the 
form \(\mathcal{N}(\bm{W} \given \bm{0}, \bm{I})\).

To find the marginal distribution outlined in Equation~\ref{eqn:spp_gp_marginal}, it's form may 
first be simplified as follows in Equation~\ref{eqn:spp_gp_marginal_simplify}.

\begin{align}
  \label{eqn:spp_gp_marginal_simplify}
  % Line 1.
  P(\bm{Y} \given \bm{X}, \beta) ={}& \int \mathcal{N}(\bm{Y} \given \bm{WX}^{T}, \beta^{-1}\bm{I})
  \mathcal{N}(\bm{W} \given \bm{0}, \bm{I}) \intd{\bm{W}} \\
  % Line 2.
  ={}& \int \frac{1}{\sqrt{\left|2\pi\beta^{-1}\bm{I}\right|}} e^{ 
    -\frac{\beta}{2} {\big( \bm{Y} - \bm{WX} \big)}^{T} \big( \bm{Y} - \bm{WX} \big) 
  }
  \frac{1}{\sqrt{\left|2\pi\bm{I}\right|}} e^{ 
    -\frac{1}{2} \bm{W}^{T}\bm{W}} \intd{\bm{W}} \\
  % Line 3.
  ={}& \frac{1}{\sqrt{\left|2\pi\beta^{-1}\bm{I}\right|}} \frac{1}{\sqrt{\left|2\pi\bm{I}\right|}} 
  \int e^{
    -\frac{\beta}{2} {\big( \bm{Y} - \bm{WX} \big)}^{T} \big( \bm{Y} - \bm{WX} \big) 
    -\frac{1}{2} \bm{W}^{T}\bm{W}} \intd{\bm{W}} \\
  % Line 4.
  \propto& \int e^{
    -\frac{\beta}{2} {\big( \bm{Y} - \bm{WX} \big)}^{T} \big( \bm{Y} - \bm{WX} \big) 
    -\frac{1}{2} \bm{W}^{T}\bm{W}} \intd{\bm{W}} \\
  % Line 5.
  \propto& \int e^{ 
  -\frac{1}{2} \Big[
    \beta {\big( \bm{Y} - \bm{X}^{T}\bm{W} \big)}^{T} \big( \bm{Y} - \bm{X}^{T}\bm{W} \big) 
    + \bm{W}^{T}\bm{W}
  \Big]} \intd{\bm{W}} \\
  % Line 6.
  \propto& e^{ -\frac{\beta}{2} \bm{Y}^{T}\bm{Y}} 
  \int e^{
  -\frac{1}{2} \Big[
    -\beta \big( \bm{Y}^{T}\bm{X}^{T}\bm{W} \big) 
    -\beta {\big( \bm{W}^{T}\bm{XY} \big)}^{T}
    +\beta \bm{W}^{T}\bm{XX}^{T}\bm{W}
    + \bm{W}^{T}\bm{W}\Big]} \intd{\bm{W}} \\
  % Line 7.
  \propto& e^{-\frac{\beta}{2} \bm{Y}^{T}\bm{Y}} 
  \int e^{
  -\frac{1}{2} \Big[
    -2\beta \bm{Y}^{T}\bm{X}^{T}\bm{W}
    +\beta \bm{W}^{T}\bm{XX}^{T}\bm{W}
    + \bm{W}^{T}\bm{W}
  \Big]} \intd{\bm{W}} \\
  % Line 7.
  \propto& e^{-\frac{\beta}{2} \bm{Y}^{T}\bm{Y}} 
  \int e^{
  -\frac{1}{2} \Big[
    \bm{W}^{T} \big( \beta \bm{XX}^{T} + \bm{I} \big) \bm{W}
    -2\beta \bm{Y}^{T}\bm{X}^{T}\bm{W}
  \Big]} \intd{\bm{W}}
\end{align}

To make the integral over \(\bm{W}\) tractable in Equation~\ref{eqn:spp_gp_marginal_simplify}, 
the distribution \(P(\bm{Y} \given \bm{X}, \beta)\) must be of a Gaussian form; an exponential 
of a quadratic form. Completing the square in \(\bm{W}\) allows the marginal to be expressed 
in such a form. First, a change of variables is made, as in Equation~\ref{eqn:spp_gp_marginal_change}.
\begin{align}
  \label{eqn:spp_gp_marginal_change}
  \bm{A} ={}& \beta \bm{XX}^{T} + \bm{I}\\
  \bm{b} ={}& \beta \bm{Y}^{T} \bm{X}^{T}
\end{align}

The procedure to integrate over \( \bm{W} \) and transform \( P(\bm{Y} \given \bm{X}, \beta) \) 
into a valid multivariate Gaussian form is as follows in Equation~\ref{eqn:spp_gp_complete}.
\begin{align}
  \label{eqn:spp_gp_complete}
  % Line 1.
  P(\bm{Y} \given \bm{X}, \beta) \propto{}& e^{-\frac{\beta}{2}\bm{Y}^{T}\bm{Y}}
  \int e^{-\frac{1}{2} 
  \big[
    \bm{W}^{T}\bm{AW} - 2\bm{bW}  
  \big]} \intd{\bm{W}}\\
  % Line 2.
  \propto& e^{-\frac{\beta}{2}\bm{Y}^{T}\bm{Y}}
  \int e^{ 
    -\frac{1}{2} \bm{W}^{T}\bm{AW} 
    - 2\bm{bW} 
    - \bm{b}^{T}\bm{A}^{-1}\bm{b}} \intd{\bm{W}}\\
  % Line 3.
  \propto& e^{-\frac{\beta}{2}\bm{Y}^{T}\bm{Y}}
  \int e^{ 
    -\frac{1}{2} \bm{W}^{T}\bm{AW} 
    - 2\bm{bA}\bm{A}^{-1}\bm{W}
    + \bm{b}^{T}\bm{A}^{-1}\bm{AA}^{-1}\bm{b}} \intd{\bm{W}}\\
  % Line 4.
  \propto& e^{-\frac{\beta}{2}\bm{Y}^{T}\bm{Y}}
  \int e^{-\frac{1}{2} \big[ 
      {\big( \bm{W} - \bm{A}^{-1}\bm{b} \big)}^{T}
      \bm{A}
      \big( \bm{W} - \bm{A}^{-1}\bm{b} \big)
      - \bm{b}^{T}\bm{A}^{-1}\bm{b}
    \big]} \intd{\bm{W}}\\
  % Line 5.
  \propto& e^{-\frac{\beta}{2}\bm{Y}^{T}\bm{Y}}
  e^{-\frac{1}{2} \big[
      \sqrt{\left| 2 \pi \bm{A} \right|}
      - \bm{b}^{T}\bm{A}^{-1}\bm{b}
    \big]}\\
  % Line 6.
  \propto& e^{\frac{1}{2} \big[
    \beta \bm{Y}^{T} \beta \bm{Y}
    - \bm{b}^{T}\bm{A}^{-1}\bm{b}
    \big]}\\
  % Line 7.
  \propto& e^{-\frac{1}{2}
  \bm{Y}^{T} \big(
    \beta \bm{I} - \beta^{2}\bm{X}^{T}\bm{A}^{-1}\bm{X}
    \big)\bm{Y}}
\end{align}

With the distribution \(P(\bm{Y} \given \bm{X}, \beta)\) derived as being proportional to  
an exponentiated quadratic form as in Equation~\ref{eqn:spp_gp_complete}, it is 
clear that the inverse covariance matrix \(\bm{\Sigma}^{-1}\) of the Gaussian distribution 
corresponding to \(P(\bm{Y} \given \bm{X}, \beta)\) is as follows in Equation 
~\ref{eqn:spp_sig_inv}.
\begin{align}
  \label{eqn:spp_sig_inv}
  % Line 1.
  \bm{\Sigma}^{-1} ={}& \beta \bm{I} - \beta^{2} \bm{X}^{T} \bm{A}^{-1} \bm{X}\\
  % Line 2.
  ={}& \beta \bm{I} - \beta^{2} \bm{X}^{T} {\big(\beta \bm{XX}^{T} + \bm{I} \big)}^{-1}
\end{align}

To obtain the covariance matrix of the distribution \(P(\bm{Y} \given \bm{X}, \beta)\), the 
form if it's inverse may be simplified by the use of the Matrix Inversion Lemma (also known 
as the Woodbury Identity)~\cite{}. First making a change of variables in the Woodbury Identity 
as in Equation~\ref{eqn:spp_woodbury}.
\begin{align}
  % Variable change.
  \label{eqn:spp_sig_inv_simp}
    \bm{A} ={}& \beta^{-1}\bm{I}\\
    \bm{C} ={}& \bm{I}\\
    \bm{U} ={}& \bm{X}^{T}\\
    \bm{V} ={}& \bm{X}
  \shortintertext{in}
  % Woodbury.
  (\bm{A} + \bm{UCV}) ={}&
  \bm{A}^{-1} - \bm{A}^{-1}\bm{U} 
  {(\bm{C}^{-1} + \bm{VA}^{-1}\bm{U})}^{-1}
  \bm{VA}^{-1}
\end{align}

The simplified form of \( \bm{\Sigma}^{-1} \) is thus given as follows in 
Equation~\ref{eqn:spp_sig_inv_simp}.
\begin{align}
  \label{eqn:spp_sig_inv_simp}
  % Line 1.
  \bm{\Sigma}^{-1} ={}& \beta \bm{I} - \beta^{2} \bm{X}^{T} 
  {\big(\beta \bm{XX}^{T} + \bm{I} \big)}^{-1}\\
  % Line 2.
  ={}& \beta^{-1} \bm{I} + \bm{X}^{T}\bm{X}
\end{align}

It follows from Equation~\ref{eqn:spp_sig_inv_simp}, that the covariance matrix \( \bm{\Sigma} \)
takes the following form.
\begin{align}
  \label{eqn:spp_sig}
  % Line 1.
  \bm{\Sigma} ={}& {(\bm{\Sigma}^{-1})}^{-1}\\
  % Line 2.
  ={}& \bm{X}^{T}\bm{X} + \beta^{-1}\bm{I}
\end{align}

As such, the form of the normalized marginal likelihood \(P(\bm{Y} \given \bm{X}, \beta)\) of 
\( \bm{W} \) is given in Equation~\ref{eqn:spp_gp_marginal_simp}.
\begin{align}
  \label{eqn:spp_gp_marginal_simp}
  % Line 1.
  P(\bf{Y \given \bm{X}, \beta}) ={}& \mathcal{N}(\bm{Y} \given \bm{0}, 
  \bm{X}^{T}\bm{X} + \beta^{-1}\bm{I})\\
  % Line 2.
  ={}& TODO
\end{align}

\subsubsection{Gaussian Process Fitting}
~\label{subsubsec:spp_gp_fitting}
As outlined in Section~\ref{subsec:spp_gplvm}, the given formulation of the marginal likelihood 
in Equation~\ref{eqn:spp_gp_marginal_simp} can be optimised directly for the latent embedding 
\( \bm{X} \). For a mapping from \(\mathbb{R}^{N \times D} \) space to \(\mathbb{R}^{N \times P} \)
space (for \( P < D \)), the latent embedding \( \bm{X} \) may be initialized by applying an 
orthogonal linear transform to the observed data and reducing dimensionality. One such approach 
is to apply Principal Component Analysis (PCA) to the observed data, taking the first \(P\) 
reverse sorted Eigenvalues of the covariance matrix.

Additionally, in the nonlinear case, where the covariance matrix \( \bm{\Sigma} \) is generated by 
a given kernel function \( \bm{\kappa} \), the hyperparameters of \( \bm{\kappa} \) may also be 
optimised.

To find the most probable latent space embedding, the latent variables \( \bm{X} \) may be 
found by directly optimising the marginal likelihood of Equation~\ref{eqn:spp_gp_marginal_simp}. 
As such, the natural logarithm may also be optimised for the latent variables \( \bm{X} \) and 
is given in Equation~\ref{eqn:eqn:spp_gp_marginal_log}.
\begin{align}
  \label{eqn:spp_gp_marginal_log}
  % Line 1.
  \mathcal{L} ={}& -\frac{DN}{2} \ln(2\pi)
  -\frac{D}{2} \ln(\left| \bm{\Sigma} \right|)
  -\frac{1}{2} \text{tr}(\bm{\Sigma}^{-1} \bm{YY}^{T})\\
  % Line 2.
  ={}& -\frac{DN}{2} \ln(2\pi)
  -\frac{D}{2} \ln(\left| \bm{\Sigma} \right|)
  -\frac{1}{2} \bm{Y}^{T}\bm{\Sigma}\bm{Y}
\end{align}

The gradient of the log marginal of Equation~\ref{eqn:spp_gp_marginal_log} 
is derived as follows in Equation~\ref{eqn:spp_gp_log_marginal_grad}.
\begin{align}
  \label{eqn:spp_gp_log_marginal_grad}
  % Line 1.
  \frac{\partial \mathcal{L}}{\partial \bm{X}} ={}&
  -\frac{1}{2} \Bigg[
    \Big( D \frac{\partial}{\partial \bm{\Sigma}} 
    \ln \big( \left| \bm{\Sigma} \right| \big) \Big) 
    \frac{\partial \bm{\Sigma}}{\partial \bm{X}}
    + \Big( \frac{\partial}{\partial \bm{\Sigma}}
    \bm{Y}^{T} \bm{\Sigma}^{-1} \bm{Y} \Big)
    \frac{\partial \bm{\Sigma}}{\partial \bm{X}}
  \Bigg]\\
  % Line 2.
  ={}& -\frac{1}{2} \Bigg[
    D \bm{\Sigma}^{-1} \frac{\partial \bm{\Sigma}}{\partial \bm{X}}
    - \bm{\Sigma}^{-1} \bm{YY}^{T} \bm{\Sigma}^{-1} 
    \frac{\partial \bm{\Sigma}}{\partial \bm{X}}
  \Bigg]\\
  % Line 3.
  ={}& -\frac{1}{2} \Bigg[
    D \bm{\Sigma}^{-1} 2 \bm{X}
    - \bm{\Sigma}^{-1} \bm{YY}^{T} \bm{\Sigma}^{-1} 2 \bm{X}
  \Bigg]\\
  % Line 4.
  ={}& -D \bm{\Sigma}^{-1} \bm{X}
  + \bm{\Sigma}^{-1} \bm{YY}^{T} \bm{\Sigma}^{-1} \bm{X}
\end{align}

The gradient derived in Equation~\ref{eqn:spp_gp_log_marginal_grad} holds 
for the case when \( \bm{\Sigma} = \bm{X}^{T}\bm{X} + \beta^{-1} \bm{I} \). 
However, for \( \bm{\Sigma} = \bm{\kappa}(.) \) where \( \bm{\kappa} \) is 
a given kernel function, the result of the derivation of Equation
~\ref{eqn:spp_gp_log_marginal_grad} is applicable. When subsituting 
\( \frac{\partial \bm{\Sigma}}{\partial} \bm{X} \) with 
\( \frac{\partial \bm{\kappa}}{\partial \bm{X}} \), the gradient is thus 
given in Equation~\ref{eqn:spp_gp_log_marginal_grad_kernel}.
\begin{equation}
  \label{eqn:spp_gp_log_marginal_grad_kernel}
  \frac{\partial \mathcal{L}}{\partial \bm{X}} = 
  -\frac{1}{2} \Bigg[
    D \bm{\Sigma}^{-1} \frac{\partial \bm{\kappa}}{\partial \bm{X}}
    - \bm{\Sigma}^{-1} \bm{YY}^{T} \bm{\Sigma}^{-1} 
    \frac{\partial \bm{\kappa}}{\partial \bm{X}}
  \Bigg]
\end{equation}

It should be noted that the gradient \( \frac{\partial \bm{\kappa}}{\partial \bm{X}} \)
may also be substitued for \( \frac{\partial \bm{\kappa}}{\partial \theta} \), for 
some hyperparameter \( \theta \) of the kernel \( \bm{\kappa} \).

A common nonlinear covariance kernel function in the Gaussian Process literature 
is the Exponentiated Quadratic \cite{EXPQUAD}, which takes the form given in 
Equation~\ref{eqn:spp_exp_quad}.
\begin{align}
  \label{eqn:spp_exp_quad}
  % Line 1.
  \bm{\kappa} \big( \bm{x}_{i}, \bm{x}_{j}, \theta_{0}, 
  \theta_{1}, \theta_{2}, \lambda \big) ={}&
  \theta_{0} e^{-\frac{\lambda}{2} 
  \left\lVert \bm{x}_{i} - \bm{x}_{j} \right\rVert^{2}}
  + \theta_{1} + \theta_{2} \delta \big( -\frac{\lambda}{2} 
  \left\lVert \bm{x}_{i} - \bm{x}_{j} \right\rVert^{2} \big)\\
  % Line 2.
  ={}& \theta_{0} e^{-\frac{\lambda}{2} 
  \sum_{n}^{D} {\big( \bm{x}_{i, n} - \bm{x}_{j, n} \big)}^{2}}
  + \theta_{1} + \theta_{2} \delta \big( -\frac{\lambda}{2} 
  \sum_{n}^{D} {\big( \bm{x}_{i, n} - \bm{x}_{j, n} \big)}^{2} \big)
\end{align}

The gradient of the Exponentiated Quadratic kernel of Equation~\ref{eqn:spp_exp_quad},
\( \frac{\partial \bm{\kappa}}{\partial \bm{x}_{i, n}} \) for the \( n^{th} \) variable 
of \( \bm{x}_{i} \) can be derived as follows in Equation~\ref{eqn:exp_quad_grad_x}.
\begin{align}
  \label{eqn:exp_quad_grad_x}
  % Line 1.
  \frac{\partial \bm{\kappa}}{\partial \bm{x}_{i, n}} ={}& 
  \frac{\partial}{\partial \bm{x}_{i, n}} \theta_{0} e^{-\frac{\lambda}{2} 
  \sum_{n = 0}^{D} -\frac{\lambda}{2} {\big( \bm{x}_{i, n} - \bm{x}_{j, n} \big)}^{2}}\\
  % Line 2.
  ={}& \frac{\partial}{\partial \bm{x}_{i, n}} \theta_{0} e^{-\frac{\lambda}{2} 
  \sum_{n = 0}^{D} {\big( \bm{x}_{i, n} - \bm{x}_{j, n} \big)}^{2}} 
  \frac{\partial}{\partial \bm{x}_{i, n}} \sum_{n = 0}^{D} -\frac{\lambda}{2} 
  {\big( \bm{x}_{i, n} - \bm{x}_{j, n} \big)}^{2}\\
  % Line 3.
  ={}& \theta_{0} e^{-\frac{\lambda}{2} 
  \sum_{n = 0}^{D} {\big( \bm{x}_{i, n} - \bm{x}_{j, n} \big)}^{2}} 
  \frac{\partial}{\partial \bm{x}_{i, n}} \sum_{n = 0}^{D} -\frac{\lambda}{2} 
  {\big( \bm{x}_{i, n} - \bm{x}_{j, n} \big)}^{2}\\
  % Line 4.
  ={}& \lambda \theta_{0} e^{-\frac{\lambda}{2} 
  \sum_{n = 0}^{D} {\big( \bm{x}_{i, n} - \bm{x}_{j, n} \big)}^{2}} 
  {\big( \bm{x}_{i, n} - \bm{x}_{j, n} \big)}
\end{align}

Following the derivation of Equation~\ref{eqn:exp_quad_grad_x}, the remaining 
gradients of \( \kappa(.) \) may be trivially derived, as in Equation
~\ref{eqn:exp_quad_grad_rest}.
\begin{align}
  \label{eqn:exp_quad_grad_rest}
  % Line 1.
  \frac{\partial \bm{\kappa}}{\partial \bm{x}_{j, n}} ={}& 
  -\frac{\partial \bm{\kappa}}{\partial \bm{x}_{i, n}}\\
  % Line 2.
  \frac{\partial \bm{\kappa}}{\partial \theta_{0}} ={}&
  e^{-\frac{\lambda}{2} 
  \sum_{n = 0}^{D} -\frac{\lambda}{2} {\big( \bm{x}_{i, n} - \bm{x}_{j, n} \big)}^{2}}\\
  % Line 3.
  \frac{\partial \bm{\kappa}}{\partial \theta_{1}} ={}& 1\\
  % Line 4.
  \frac{\partial \bm{\kappa}}{\partial \theta_{2}} ={}& 0\\
  % Line 5.
  \frac{\partial \bm{\kappa}}{\partial \lambda} ={}& \text{TODO}
\end{align}

\subsection{Latent Space Shape Estimation}
~\label{sub:spp_latent_shape_est}
The form of the model outlined in Section~\ref{subsubsec:spp_gp_marginal_likelihood}
when trained as outlined in Section~\ref{subsubsec:spp_gp_fitting} defines a Gaussian 
Process prior over the latent space embedding \( \bm{X} \). When regressing shape, it 
is desirable to predict the 3D shape of a given latent space point, conditioned on this 
prior. This conditioning yields a posterior mean estimation over 3D shape for a given 
latent space point.

The estimated posterior mean provides a DCT compressed representation of an SDF shape 
volume \( \bm{\Phi} \in \mathbb{R}^{N \times N \times N} \). The DCT compressed form 
of \( \bm{\Phi} \) given by the aforementioned posterior mean is obtained by Gaussian 
Process Regression~\cite{}. Finally, the true, uncompressed form of \( \bm{\Phi} \) is 
obtained by taking the IDCT of the posterior mean. The granularity of the geometric 
properties captured by the GP shape prior is governed by the number of DCT harmonics 
used in the compression and decompression processes at training and prediction.

\subsubsection{Shape Posterior Mean Estimation}
~\label{subsubsec:spp_pos_mean_est}
With the optimised latent variables \( \bm{X} \), the formulation outlined in 
Equation~\ref{eqn:spp_gp_marginal_simp} defines a Gaussian Process prior 
over functions of \( \bm{X} \), as follows.
\begin{equation}
  \label{eqn:gp_prior}
  \bm{f}(\bm{X}) \sim \mathcal{GP}(\bm{0}, \bm{\Sigma}_{\bm{XX}})
\end{equation}

A Gaussian Process prior may similarly be constructed for observed latent 
space points \( \bm{L} \), as follows.
\begin{equation}
  \label{eqn:gp_prior_latent}
  \bm{f}_{\star}(\bm{l}) \sim \mathcal{GP}(\bm{0}, \bm{\Sigma}_{\bm{LL}})
\end{equation}

It follows from Equations~\ref{eqn:gp_prior} and~\ref{eqn:gp_prior_latent} that 
the joint distribution over \( \bm{X} \) and \( \bm{L} \) can be formulated as 
follows in Equation~\ref{eqn:spp_gp_joint}.
\begin{equation}
  \label{eqn:spp_gp_joint}
  \begin{bmatrix}
    \bm{f}\\
    \bm{f}_{\star}
  \end{bmatrix}
  \sim \mathcal{N} \Bigg(
    \begin{bmatrix}
      \bm{0}\\
      \bm{0}
    \end{bmatrix},
    \begin{bmatrix}
      \bm{\Sigma}_{\bm{XX}} & \bm{\Sigma}_{\bm{XL}}\\
      \bm{\Sigma}_{\bm{LX}} & \bm{\Sigma}_{\bm{LL}}
    \end{bmatrix}
  \Bigg)
\end{equation}

The posterior of \( \bm{f}_{\star} \) conditioned on \( \bm{f} \) is given by
the distribution in Equation~\ref{eqn:spp_gp_conditional}.
\begin{equation}
  \label{eqn:spp_gp_conditional}
  P(\bm{f}_{\star} \given \bm{X}, \bm{L}, \bm{f}) = 
  \mathcal{N} \Big(
    \bm{\Sigma}_{LX} \bm{\Sigma}_{XX}^{-1} \bm{f}, 
    \bm{\Sigma}_{LL} - \bm{\Sigma}_{LX} \bm{\Sigma}_{XX}^{-1} \bm{\Sigma}_{XL}
  \Big) 
\end{equation}

As such, to obtain a posterior mean prediction for a latent space point \( \bm{l} \), 
a draw from the distribution outlined in Equation~\ref{eqn:spp_gp_conditional} is 
taken as follows.
\begin{equation}
  \label{eqn:spp_gp_posterior_draw}
  \bm{f}_{\star} \sim P(\bm{f}_{\star} \given \bm{X}, \bm{L}, \bm{f})
\end{equation}

\subsubsection{Signed Distance Function Extraction}
~\label{subsubsec:sdf_extraction}
As outlined in Section~\ref{sub:spp_latent_shape_est}, the SDF volume \( \bm{\Phi} \) 
is generated by taking the IDCT of the posterior mean of the GP prior consitioned on 
a given latent space point, as given by Equation~\ref{eqn:spp_gp_conditional}. 

The latent variable model outlined in Section~\ref{subsec:spp_gplvm} is a distribution 
over a latent space embedding of 3D shape. However, the 3D shape data on which the GP 
prior was modelled is compressed with the DCT. The intuition bedind the use of the DCT 
is the property that the number of harmonics used in the transform impacts on the 
granularity of the resultant 3D shape geometry. It has been shown~\cite{REN} that the 
lower frequency harmonics of the DCT capture general structure and shape, whilst higher 
frequency harmonics capture finer detailed geometric features. 

The DCT for 3D volume data is given in Equation~\ref{eqn:spp_dct}.
\begin{equation}
  \label{eqn:spp_dct}
  a
\end{equation}
% on prediction, IDCT used (same harmonics) - maybe show some figures??
% give IDCT and gradient (for backpropagation).
% show grad of idct is idct(grad)

\subsection{Pose Estimation}
~\label{subsec:spp_pose_estim}
% same rodriguez paramaterisation as before - refer back
% rotation on interval (0, 2pi]
% same trans and grad
% aggregate/mean raycast points

\subsection{Rendering}
~\label{subsec:rendering}
% refer to prev(still to write) raycast
% differentiable by...
% refer back to log sigmoid and derivative
% give pseudocode
% example probability map

\subsection{Loss Function and Backpropagation}
~\label{subsec:spp_loss}

\section{Qualitative Results}
~\label{sec:spp_qualitative}

\section{Quantitative Results}
~\label{sec:spp_quantitative}

\section{Discussion}
~\label{sec:spp_discussion}